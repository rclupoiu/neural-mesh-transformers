{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, LayerNorm, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('/workspace/data_gen/futurepred_graphs.pt')[:300]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, args, emb=False):\n",
    "        super(MeshGraphNet, self).__init__()\n",
    "        \"\"\"\n",
    "        MeshGraphNet model. This model is built upon Deepmind's 2021 paper.\n",
    "        This model consists of three parts: (1) Preprocessing: encoder (2) Processor\n",
    "        (3) postproccessing: decoder. Encoder has an edge and node decoders respectively.\n",
    "        Processor has two processors for edge and node respectively. Note that edge attributes have to be\n",
    "        updated first. Decoder is only for nodes.\n",
    "\n",
    "        Input_dim: dynamic variables + node_type + node_position\n",
    "        Hidden_dim: 128 in deepmind's paper\n",
    "        Output_dim: dynamic variables: velocity changes (1)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        # encoder convert raw inputs into latent embeddings\n",
    "        self.node_encoder = Sequential(Linear(input_dim_node , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim))\n",
    "\n",
    "        self.edge_encoder = Sequential(Linear( input_dim_edge , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "        self.processor = nn.ModuleList()\n",
    "        assert (self.num_layers >= 1), 'Number of message passing layers is not <1'\n",
    "\n",
    "        processor_layer=self.build_processor_model()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.processor.append(processor_layer(hidden_dim,hidden_dim))\n",
    "\n",
    "\n",
    "        # decoder: only for node embeddings\n",
    "        self.decoder = Sequential(Linear( hidden_dim , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, output_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "    def build_processor_model(self):\n",
    "        return ProcessorLayer\n",
    "\n",
    "\n",
    "    def forward(self,data):\n",
    "        \"\"\"\n",
    "        Encoder encodes graph (node/edge features) into latent vectors (node/edge embeddings)\n",
    "        The return of processor is fed into the processor for generating new feature vectors\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # Step 1: encode node/edge features into latent node/edge embeddings\n",
    "        x = self.node_encoder(x) # output shape is the specified hidden dimension\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr) # output shape is the specified hidden dimension\n",
    "\n",
    "        # step 2: perform message passing with latent node/edge embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x,edge_attr = self.processor[i](x,edge_index,edge_attr)\n",
    "\n",
    "        # step 3: decode latent node embeddings into physical quantities of interest\n",
    "\n",
    "        return self.decoder(x)\n",
    "\n",
    "    # def loss(self, pred, inputs,mean_vec_y,std_vec_y):\n",
    "    #     #Define the node types that we calculate loss for\n",
    "    #     normal=torch.tensor(0)\n",
    "    #     outflow=torch.tensor(5)\n",
    "\n",
    "    #     #Get the loss mask for the nodes of the types we calculate loss for\n",
    "    #     loss_mask=torch.logical_or((torch.argmax(inputs.x[:,2:],dim=1)==torch.tensor(0)),\n",
    "    #                                (torch.argmax(inputs.x[:,2:],dim=1)==torch.tensor(5)))\n",
    "\n",
    "    #     #Normalize labels with dataset statistics\n",
    "    #     labels = normalize(inputs.y,mean_vec_y,std_vec_y)\n",
    "\n",
    "    #     #Find sum of square errors\n",
    "    #     error=torch.sum((labels-pred)**2,axis=1)\n",
    "\n",
    "    #     #Root and mean the errors for the nodes we calculate loss for\n",
    "    #     loss=torch.sqrt(torch.mean(error[loss_mask]))\n",
    "        \n",
    "    #     return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge message passing, aggregation, and passing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,  **kwargs):\n",
    "        super(ProcessorLayer, self).__init__(  **kwargs )\n",
    "        \"\"\"\n",
    "        in_channels: dim of node embeddings [128], out_channels: dim of edge embeddings [128]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Note that the node and edge encoders both have the same hidden dimension\n",
    "        # size. This means that the input of the edge processor will always be\n",
    "        # three times the specified hidden dimension\n",
    "        # (input: adjacent node embeddings and self embeddings)\n",
    "        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        reset parameters for stacked MLP layers\n",
    "        \"\"\"\n",
    "        self.edge_mlp[0].reset_parameters()\n",
    "        self.edge_mlp[2].reset_parameters()\n",
    "\n",
    "        self.node_mlp[0].reset_parameters()\n",
    "        self.node_mlp[2].reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, size = None):\n",
    "        \"\"\"\n",
    "        Handle the pre and post-processing of node features/embeddings,\n",
    "        as well as initiates message passing by calling the propagate function.\n",
    "\n",
    "        Note that message passing and aggregation are handled by the propagate\n",
    "        function, and the update\n",
    "\n",
    "        x has shpae [node_num , in_channels] (node embeddings)\n",
    "        edge_index: [2, edge_num]\n",
    "        edge_attr: [E, in_channels]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        out, updated_edges = self.propagate(edge_index, x = x, edge_attr = edge_attr, size = size) # out has the shape of [E, out_channels]\n",
    "\n",
    "        updated_nodes = torch.cat([x,out],dim=1)        # Complete the aggregation through self-aggregation\n",
    "\n",
    "        updated_nodes = x + self.node_mlp(updated_nodes) # residual connection\n",
    "\n",
    "        return updated_nodes, updated_edges\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        source_node: x_i has the shape of [E, in_channels]\n",
    "        target_node: x_j has the shape of [E, in_channels]\n",
    "        target_edge: edge_attr has the shape of [E, out_channels]\n",
    "\n",
    "        The messages that are passed are the raw embeddings. These are not processed.\n",
    "        \"\"\"\n",
    "\n",
    "        updated_edges=torch.cat([x_i, x_j, edge_attr], dim = 1) # tmp_emb has the shape of [E, 3 * in_channels]\n",
    "        updated_edges=self.edge_mlp(updated_edges)+edge_attr\n",
    "\n",
    "        return updated_edges\n",
    "\n",
    "    def aggregate(self, updated_edges, edge_index, dim_size = None):\n",
    "        \"\"\"\n",
    "        First we aggregate from neighbors (i.e., adjacent nodes) through concatenation,\n",
    "        then we aggregate self message (from the edge itself). This is streamlined\n",
    "        into one operation here.\n",
    "        \"\"\"\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = 0\n",
    "\n",
    "        out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')\n",
    "\n",
    "        return out, updated_edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "        def __init__(self, d):\n",
    "            self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args in [\n",
    "            {\n",
    "            'hidden_dim': 32,\n",
    "            'num_layers': 3,\n",
    "            'batch_size': 1,\n",
    "            'lr': 0.001,\n",
    "            'opt': 'adam',\n",
    "            'opt_scheduler': 'none',\n",
    "            'opt_restart': 0,\n",
    "            'weight_decay': 5e-4,\n",
    "            'num_epochs': 5000,\n",
    "            'seed': 42,\n",
    "            'epochs': 5000,\n",
    "            },\n",
    "        ]:\n",
    "            args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "\n",
    "#Build the data loader\n",
    "#Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "num_node_features = dataset[0].x.shape[1]\n",
    "num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "num_classes = 1 #only one prediction per node: the spike value at the next time step\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "#Build the model\n",
    "model = MeshGraphNet(num_node_features, num_edge_features, \n",
    "                     args.hidden_dim, num_classes, args).to(device)\n",
    "\n",
    "\n",
    "#Build the optimizer\n",
    "scheduler, optimizer = build_optimizer(args, model.parameters())\n",
    "\n",
    "#Build the loss function\n",
    "#loss_fn = nn.NLLLoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Define a pandas dataframe to store the training results\n",
    "df = pd.DataFrame(columns=['epoch', 'loss', 'accuracy', 'test_loss', 'test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 230\n",
      "Epoch: 000, Train Loss: 0.8862426, Train Accuracy: 0.0, Test Loss: 0.8748832, Test Accuracy: 0.0\n",
      "Epoch: 1, Batch: 230\n",
      "Epoch: 001, Train Loss: 0.8624209, Train Accuracy: 0.0, Test Loss: 0.8735591, Test Accuracy: 0.0\n",
      "Epoch: 2, Batch: 100"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#Train the model\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    num_batches = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        #Print the epoch and the batch number. Erase the previous line to avoid cluttering the terminal\n",
    "        if(i%10==0):\n",
    "            print(\"\\rEpoch: %d, Batch: %d\" % (epoch, i), end='')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        #Add to the accuracy the number of correct binary predictions\n",
    "        accuracy += 0#(out.round(decimals=0) == data.y).sum().item()/len(data.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    #Find the performance on the test set\n",
    "    model.eval()\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    test_num_batches = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        test_loss += loss_fn(out, data.y).item()\n",
    "        test_accuracy += 0#(out.round(decimals=0) == data.y).sum().item()/len(data.y)\n",
    "        test_num_batches += 1\n",
    "\n",
    "    print('\\nEpoch: {:03d}, Train Loss: {:.7f}, Train Accuracy: {:.3}, Test Loss: {:.7f}, Test Accuracy: {:.3}'.format(epoch, \n",
    "            total_loss/num_batches, accuracy/num_batches, test_loss/test_num_batches, test_accuracy/test_num_batches))\n",
    "\n",
    "    #Store the results in the dataframe\n",
    "    df = pd.concat([df, pd.DataFrame({'epoch': epoch, 'loss': total_loss/num_batches, \n",
    "                                        'accuracy': accuracy/num_batches, 'test_loss': test_loss/test_num_batches,\n",
    "                                        'test_accuracy': test_accuracy/test_num_batches\n",
    "                                        }, index=[0])], ignore_index=True)\n",
    "    #Save the dataframe to a csv file\n",
    "    df.to_csv('results_forwardpred.csv', index=False)\n",
    "\n",
    "    if(epoch==0):\n",
    "        best_loss = test_loss/test_num_batches\n",
    "    if(test_loss/test_num_batches < best_loss):\n",
    "        best_loss = test_loss/test_num_batches\n",
    "        torch.save(model.state_dict(), 'model_forwardpred.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
