{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, LayerNorm, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('/workspace/data_gen/futurepred_graphs.pt')[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, args, emb=False):\n",
    "        super(MeshGraphNet, self).__init__()\n",
    "        \"\"\"\n",
    "        MeshGraphNet model. This model is built upon Deepmind's 2021 paper.\n",
    "        This model consists of three parts: (1) Preprocessing: encoder (2) Processor\n",
    "        (3) postproccessing: decoder. Encoder has an edge and node decoders respectively.\n",
    "        Processor has two processors for edge and node respectively. Note that edge attributes have to be\n",
    "        updated first. Decoder is only for nodes.\n",
    "\n",
    "        Input_dim: dynamic variables + node_type + node_position\n",
    "        Hidden_dim: 128 in deepmind's paper\n",
    "        Output_dim: dynamic variables: velocity changes (1)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        # encoder convert raw inputs into latent embeddings\n",
    "        self.node_encoder = Sequential(Linear(input_dim_node , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim))\n",
    "\n",
    "        self.edge_encoder = Sequential(Linear( input_dim_edge , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, hidden_dim),\n",
    "                              LayerNorm(hidden_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "        self.processor = nn.ModuleList()\n",
    "        assert (self.num_layers >= 1), 'Number of message passing layers is not <1'\n",
    "\n",
    "        processor_layer=self.build_processor_model()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.processor.append(processor_layer(hidden_dim,hidden_dim))\n",
    "\n",
    "\n",
    "        # decoder: only for node embeddings\n",
    "        self.decoder = Sequential(Linear( hidden_dim , hidden_dim),\n",
    "                              ReLU(),\n",
    "                              Linear( hidden_dim, output_dim)\n",
    "                              )\n",
    "\n",
    "\n",
    "    def build_processor_model(self):\n",
    "        return ProcessorLayer\n",
    "\n",
    "\n",
    "    def forward(self,data):\n",
    "        \"\"\"\n",
    "        Encoder encodes graph (node/edge features) into latent vectors (node/edge embeddings)\n",
    "        The return of processor is fed into the processor for generating new feature vectors\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # Step 1: encode node/edge features into latent node/edge embeddings\n",
    "        x = self.node_encoder(x) # output shape is the specified hidden dimension\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr) # output shape is the specified hidden dimension\n",
    "\n",
    "        # step 2: perform message passing with latent node/edge embeddings\n",
    "        for i in range(self.num_layers):\n",
    "            x,edge_attr = self.processor[i](x,edge_index,edge_attr)\n",
    "\n",
    "        # step 3: decode latent node embeddings into physical quantities of interest\n",
    "\n",
    "        return self.decoder(x)\n",
    "\n",
    "    # def loss(self, pred, inputs,mean_vec_y,std_vec_y):\n",
    "    #     #Define the node types that we calculate loss for\n",
    "    #     normal=torch.tensor(0)\n",
    "    #     outflow=torch.tensor(5)\n",
    "\n",
    "    #     #Get the loss mask for the nodes of the types we calculate loss for\n",
    "    #     loss_mask=torch.logical_or((torch.argmax(inputs.x[:,2:],dim=1)==torch.tensor(0)),\n",
    "    #                                (torch.argmax(inputs.x[:,2:],dim=1)==torch.tensor(5)))\n",
    "\n",
    "    #     #Normalize labels with dataset statistics\n",
    "    #     labels = normalize(inputs.y,mean_vec_y,std_vec_y)\n",
    "\n",
    "    #     #Find sum of square errors\n",
    "    #     error=torch.sum((labels-pred)**2,axis=1)\n",
    "\n",
    "    #     #Root and mean the errors for the nodes we calculate loss for\n",
    "    #     loss=torch.sqrt(torch.mean(error[loss_mask]))\n",
    "        \n",
    "    #     return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge message passing, aggregation, and passing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,  **kwargs):\n",
    "        super(ProcessorLayer, self).__init__(  **kwargs )\n",
    "        \"\"\"\n",
    "        in_channels: dim of node embeddings [128], out_channels: dim of edge embeddings [128]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Note that the node and edge encoders both have the same hidden dimension\n",
    "        # size. This means that the input of the edge processor will always be\n",
    "        # three times the specified hidden dimension\n",
    "        # (input: adjacent node embeddings and self embeddings)\n",
    "        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        reset parameters for stacked MLP layers\n",
    "        \"\"\"\n",
    "        self.edge_mlp[0].reset_parameters()\n",
    "        self.edge_mlp[2].reset_parameters()\n",
    "\n",
    "        self.node_mlp[0].reset_parameters()\n",
    "        self.node_mlp[2].reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, size = None):\n",
    "        \"\"\"\n",
    "        Handle the pre and post-processing of node features/embeddings,\n",
    "        as well as initiates message passing by calling the propagate function.\n",
    "\n",
    "        Note that message passing and aggregation are handled by the propagate\n",
    "        function, and the update\n",
    "\n",
    "        x has shpae [node_num , in_channels] (node embeddings)\n",
    "        edge_index: [2, edge_num]\n",
    "        edge_attr: [E, in_channels]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        out, updated_edges = self.propagate(edge_index, x = x, edge_attr = edge_attr, size = size) # out has the shape of [E, out_channels]\n",
    "\n",
    "        updated_nodes = torch.cat([x,out],dim=1)        # Complete the aggregation through self-aggregation\n",
    "\n",
    "        updated_nodes = x + self.node_mlp(updated_nodes) # residual connection\n",
    "\n",
    "        return updated_nodes, updated_edges\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        source_node: x_i has the shape of [E, in_channels]\n",
    "        target_node: x_j has the shape of [E, in_channels]\n",
    "        target_edge: edge_attr has the shape of [E, out_channels]\n",
    "\n",
    "        The messages that are passed are the raw embeddings. These are not processed.\n",
    "        \"\"\"\n",
    "\n",
    "        updated_edges=torch.cat([x_i, x_j, edge_attr], dim = 1) # tmp_emb has the shape of [E, 3 * in_channels]\n",
    "        updated_edges=self.edge_mlp(updated_edges)+edge_attr\n",
    "\n",
    "        return updated_edges\n",
    "\n",
    "    def aggregate(self, updated_edges, edge_index, dim_size = None):\n",
    "        \"\"\"\n",
    "        First we aggregate from neighbors (i.e., adjacent nodes) through concatenation,\n",
    "        then we aggregate self message (from the edge itself). This is streamlined\n",
    "        into one operation here.\n",
    "        \"\"\"\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = 0\n",
    "\n",
    "        out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')\n",
    "\n",
    "        return out, updated_edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "        def __init__(self, d):\n",
    "            self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args in [\n",
    "            {\n",
    "            'hidden_dim': 64,\n",
    "            'num_layers': 3,\n",
    "            'batch_size': 1,\n",
    "            'lr': 0.001,\n",
    "            'opt': 'adam',\n",
    "            'opt_scheduler': 'none',\n",
    "            'opt_restart': 0,\n",
    "            'weight_decay': 5e-4,\n",
    "            'num_epochs': 2000,\n",
    "            'seed': 42,\n",
    "            'epochs': 2000,\n",
    "            },\n",
    "        ]:\n",
    "            args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "\n",
    "#Build the data loader\n",
    "#Split the dataset into training and validation sets\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "num_node_features = dataset.x.shape[1]\n",
    "num_edge_features = dataset.edge_attr.shape[1]\n",
    "num_classes = 1 #only one prediction per node: the spike value at the next time step\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "#Build the model\n",
    "model = MeshGraphNet(num_node_features, num_edge_features, \n",
    "                     args.hidden_dim, num_classes, args).to(device)\n",
    "\n",
    "\n",
    "#Build the optimizer\n",
    "scheduler, optimizer = build_optimizer(args, model.parameters())\n",
    "\n",
    "#Build the loss function\n",
    "#loss_fn = nn.NLLLoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Define a pandas dataframe to store the training results\n",
    "df = pd.DataFrame(columns=['epoch', 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.7720639\n",
      "Epoch: 001, Loss: 1.1905445\n",
      "Epoch: 002, Loss: 0.7694557\n",
      "Epoch: 003, Loss: 0.8394136\n",
      "Epoch: 004, Loss: 0.7509448\n",
      "Epoch: 005, Loss: 0.7239276\n",
      "Epoch: 006, Loss: 0.7434470\n",
      "Epoch: 007, Loss: 0.7448326\n",
      "Epoch: 008, Loss: 0.7281333\n",
      "Epoch: 009, Loss: 0.7198561\n",
      "Epoch: 010, Loss: 0.7263402\n",
      "Epoch: 011, Loss: 0.7320278\n",
      "Epoch: 012, Loss: 0.7277185\n",
      "Epoch: 013, Loss: 0.7199172\n",
      "Epoch: 014, Loss: 0.7177086\n",
      "Epoch: 015, Loss: 0.7216030\n",
      "Epoch: 016, Loss: 0.7238374\n",
      "Epoch: 017, Loss: 0.7207900\n",
      "Epoch: 018, Loss: 0.7162870\n",
      "Epoch: 019, Loss: 0.7147981\n",
      "Epoch: 020, Loss: 0.7163277\n",
      "Epoch: 021, Loss: 0.7174155\n",
      "Epoch: 022, Loss: 0.7160845\n",
      "Epoch: 023, Loss: 0.7136133\n",
      "Epoch: 024, Loss: 0.7125170\n",
      "Epoch: 025, Loss: 0.7134879\n",
      "Epoch: 026, Loss: 0.7141801\n",
      "Epoch: 027, Loss: 0.7130392\n",
      "Epoch: 028, Loss: 0.7113628\n",
      "Epoch: 029, Loss: 0.7108665\n",
      "Epoch: 030, Loss: 0.7113752\n",
      "Epoch: 031, Loss: 0.7115231\n",
      "Epoch: 032, Loss: 0.7106801\n",
      "Epoch: 033, Loss: 0.7096356\n",
      "Epoch: 034, Loss: 0.7093243\n",
      "Epoch: 035, Loss: 0.7095608\n",
      "Epoch: 036, Loss: 0.7093954\n",
      "Epoch: 037, Loss: 0.7086153\n",
      "Epoch: 038, Loss: 0.7079465\n",
      "Epoch: 039, Loss: 0.7078295\n",
      "Epoch: 040, Loss: 0.7077699\n",
      "Epoch: 041, Loss: 0.7073460\n",
      "Epoch: 042, Loss: 0.7067168\n",
      "Epoch: 043, Loss: 0.7063267\n",
      "Epoch: 044, Loss: 0.7061889\n",
      "Epoch: 045, Loss: 0.7058706\n",
      "Epoch: 046, Loss: 0.7052777\n",
      "Epoch: 047, Loss: 0.7047625\n",
      "Epoch: 048, Loss: 0.7045082\n",
      "Epoch: 049, Loss: 0.7041568\n",
      "Epoch: 050, Loss: 0.7035633\n",
      "Epoch: 051, Loss: 0.7030775\n",
      "Epoch: 052, Loss: 0.7027709\n",
      "Epoch: 053, Loss: 0.7023125\n",
      "Epoch: 054, Loss: 0.7016825\n",
      "Epoch: 055, Loss: 0.7012140\n",
      "Epoch: 056, Loss: 0.7007523\n",
      "Epoch: 057, Loss: 0.7001066\n",
      "Epoch: 058, Loss: 0.6995587\n",
      "Epoch: 059, Loss: 0.6990193\n",
      "Epoch: 060, Loss: 0.6983071\n",
      "Epoch: 061, Loss: 0.6976795\n",
      "Epoch: 062, Loss: 0.6969647\n",
      "Epoch: 063, Loss: 0.6962133\n",
      "Epoch: 064, Loss: 0.6954795\n",
      "Epoch: 065, Loss: 0.6946092\n",
      "Epoch: 066, Loss: 0.6937256\n",
      "Epoch: 067, Loss: 0.6927670\n",
      "Epoch: 068, Loss: 0.6918429\n",
      "Epoch: 069, Loss: 0.6907725\n",
      "Epoch: 070, Loss: 0.6897113\n",
      "Epoch: 071, Loss: 0.6885723\n",
      "Epoch: 072, Loss: 0.6873711\n",
      "Epoch: 073, Loss: 0.6861671\n",
      "Epoch: 074, Loss: 0.6849310\n",
      "Epoch: 075, Loss: 0.6837603\n",
      "Epoch: 076, Loss: 0.6827487\n",
      "Epoch: 077, Loss: 0.6823194\n",
      "Epoch: 078, Loss: 0.6810296\n",
      "Epoch: 079, Loss: 0.6788995\n",
      "Epoch: 080, Loss: 0.6767848\n",
      "Epoch: 081, Loss: 0.6761457\n",
      "Epoch: 082, Loss: 0.6764120\n",
      "Epoch: 083, Loss: 0.6738109\n",
      "Epoch: 084, Loss: 0.6712165\n",
      "Epoch: 085, Loss: 0.6696947\n",
      "Epoch: 086, Loss: 0.6693758\n",
      "Epoch: 087, Loss: 0.6705317\n",
      "Epoch: 088, Loss: 0.6665195\n",
      "Epoch: 089, Loss: 0.6637004\n",
      "Epoch: 090, Loss: 0.6620293\n",
      "Epoch: 091, Loss: 0.6623204\n",
      "Epoch: 092, Loss: 0.6663141\n",
      "Epoch: 093, Loss: 0.6593778\n",
      "Epoch: 094, Loss: 0.6564499\n",
      "Epoch: 095, Loss: 0.6565085\n",
      "Epoch: 096, Loss: 0.6557583\n",
      "Epoch: 097, Loss: 0.6586035\n",
      "Epoch: 098, Loss: 0.6514468\n",
      "Epoch: 099, Loss: 0.6493188\n",
      "Epoch: 100, Loss: 0.6503266\n",
      "Epoch: 101, Loss: 0.6483700\n",
      "Epoch: 102, Loss: 0.6509874\n",
      "Epoch: 103, Loss: 0.6426795\n",
      "Epoch: 104, Loss: 0.6413544\n",
      "Epoch: 105, Loss: 0.6425300\n",
      "Epoch: 106, Loss: 0.6364036\n",
      "Epoch: 107, Loss: 0.6332908\n",
      "Epoch: 108, Loss: 0.6333597\n",
      "Epoch: 109, Loss: 0.6337958\n",
      "Epoch: 110, Loss: 0.6344413\n",
      "Epoch: 111, Loss: 0.6249679\n",
      "Epoch: 112, Loss: 0.6232438\n",
      "Epoch: 113, Loss: 0.6304520\n",
      "Epoch: 114, Loss: 0.6266097\n",
      "Epoch: 115, Loss: 0.6303827\n",
      "Epoch: 116, Loss: 0.6154743\n",
      "Epoch: 117, Loss: 0.6435870\n",
      "Epoch: 118, Loss: 0.6702288\n",
      "Epoch: 119, Loss: 0.6698741\n",
      "Epoch: 120, Loss: 0.6609567\n",
      "Epoch: 121, Loss: 0.6646631\n",
      "Epoch: 122, Loss: 0.6416588\n",
      "Epoch: 123, Loss: 0.6290746\n",
      "Epoch: 124, Loss: 0.6422438\n",
      "Epoch: 125, Loss: 0.6346586\n",
      "Epoch: 126, Loss: 0.6216197\n",
      "Epoch: 127, Loss: 0.6200207\n",
      "Epoch: 128, Loss: 0.6210323\n",
      "Epoch: 129, Loss: 0.6168013\n",
      "Epoch: 130, Loss: 0.6101020\n",
      "Epoch: 131, Loss: 0.6065585\n",
      "Epoch: 132, Loss: 0.6047065\n",
      "Epoch: 133, Loss: 0.6016780\n",
      "Epoch: 134, Loss: 0.5991068\n",
      "Epoch: 135, Loss: 0.5949150\n",
      "Epoch: 136, Loss: 0.5912440\n",
      "Epoch: 137, Loss: 0.5899079\n",
      "Epoch: 138, Loss: 0.5856249\n",
      "Epoch: 139, Loss: 0.5820636\n",
      "Epoch: 140, Loss: 0.5805444\n",
      "Epoch: 141, Loss: 0.5775867\n",
      "Epoch: 142, Loss: 0.5732741\n",
      "Epoch: 143, Loss: 0.5750012\n",
      "Epoch: 144, Loss: 0.5761721\n",
      "Epoch: 145, Loss: 0.5964994\n",
      "Epoch: 146, Loss: 0.5665132\n",
      "Epoch: 147, Loss: 0.6184900\n",
      "Epoch: 148, Loss: 0.6512332\n",
      "Epoch: 149, Loss: 0.6767814\n",
      "Epoch: 150, Loss: 0.6531523\n",
      "Epoch: 151, Loss: 0.6671928\n",
      "Epoch: 152, Loss: 0.6740206\n",
      "Epoch: 153, Loss: 0.6483740\n",
      "Epoch: 154, Loss: 0.6347407\n",
      "Epoch: 155, Loss: 0.6340343\n",
      "Epoch: 156, Loss: 0.6196473\n",
      "Epoch: 157, Loss: 0.6237671\n",
      "Epoch: 158, Loss: 0.6174353\n",
      "Epoch: 159, Loss: 0.6063757\n",
      "Epoch: 160, Loss: 0.5988106\n",
      "Epoch: 161, Loss: 0.5847586\n",
      "Epoch: 162, Loss: 0.5872907\n",
      "Epoch: 163, Loss: 0.5735825\n",
      "Epoch: 164, Loss: 0.5733218\n",
      "Epoch: 165, Loss: 0.5643544\n",
      "Epoch: 166, Loss: 0.5662023\n",
      "Epoch: 167, Loss: 0.5607588\n",
      "Epoch: 168, Loss: 0.5564368\n",
      "Epoch: 169, Loss: 0.5539546\n",
      "Epoch: 170, Loss: 0.5465979\n",
      "Epoch: 171, Loss: 0.5443524\n",
      "Epoch: 172, Loss: 0.5387062\n",
      "Epoch: 173, Loss: 0.5381143\n",
      "Epoch: 174, Loss: 0.5373381\n",
      "Epoch: 175, Loss: 0.5289317\n",
      "Epoch: 176, Loss: 0.5292377\n",
      "Epoch: 177, Loss: 0.5270382\n",
      "Epoch: 178, Loss: 0.5193545\n",
      "Epoch: 179, Loss: 0.5187622\n",
      "Epoch: 180, Loss: 0.5209708\n",
      "Epoch: 181, Loss: 0.5113258\n",
      "Epoch: 182, Loss: 0.5098401\n",
      "Epoch: 183, Loss: 0.5178036\n",
      "Epoch: 184, Loss: 0.5066937\n",
      "Epoch: 185, Loss: 0.5005826\n",
      "Epoch: 186, Loss: 0.4955953\n",
      "Epoch: 187, Loss: 0.4949223\n",
      "Epoch: 188, Loss: 0.5064215\n",
      "Epoch: 189, Loss: 0.5093778\n",
      "Epoch: 190, Loss: 0.5526654\n",
      "Epoch: 191, Loss: 0.5175757\n",
      "Epoch: 192, Loss: 0.5487182\n",
      "Epoch: 193, Loss: 0.4984215\n",
      "Epoch: 194, Loss: 0.5171061\n",
      "Epoch: 195, Loss: 0.4939191\n",
      "Epoch: 196, Loss: 0.4983100\n",
      "Epoch: 197, Loss: 0.5103098\n",
      "Epoch: 198, Loss: 0.5078766\n",
      "Epoch: 199, Loss: 0.4901664\n",
      "Epoch: 200, Loss: 0.4964776\n",
      "Epoch: 201, Loss: 0.5030159\n",
      "Epoch: 202, Loss: 0.4883177\n",
      "Epoch: 203, Loss: 0.4883731\n",
      "Epoch: 204, Loss: 0.4756005\n",
      "Epoch: 205, Loss: 0.4793221\n",
      "Epoch: 206, Loss: 0.4688582\n",
      "Epoch: 207, Loss: 0.4768304\n",
      "Epoch: 208, Loss: 0.4687455\n",
      "Epoch: 209, Loss: 0.4632058\n",
      "Epoch: 210, Loss: 0.4686883\n",
      "Epoch: 211, Loss: 0.4586797\n",
      "Epoch: 212, Loss: 0.4560026\n",
      "Epoch: 213, Loss: 0.4552295\n",
      "Epoch: 214, Loss: 0.4491131\n",
      "Epoch: 215, Loss: 0.4488617\n",
      "Epoch: 216, Loss: 0.4452875\n",
      "Epoch: 217, Loss: 0.4417869\n",
      "Epoch: 218, Loss: 0.4409449\n",
      "Epoch: 219, Loss: 0.4392766\n",
      "Epoch: 220, Loss: 0.4331570\n",
      "Epoch: 221, Loss: 0.4348497\n",
      "Epoch: 222, Loss: 0.4304191\n",
      "Epoch: 223, Loss: 0.4251006\n",
      "Epoch: 224, Loss: 0.4252506\n",
      "Epoch: 225, Loss: 0.4221139\n",
      "Epoch: 226, Loss: 0.4190813\n",
      "Epoch: 227, Loss: 0.4200021\n",
      "Epoch: 228, Loss: 0.4233944\n",
      "Epoch: 229, Loss: 0.4262811\n",
      "Epoch: 230, Loss: 0.4568236\n",
      "Epoch: 231, Loss: 0.4120153\n",
      "Epoch: 232, Loss: 0.4881478\n",
      "Epoch: 233, Loss: 0.6114512\n",
      "Epoch: 234, Loss: 0.6330588\n",
      "Epoch: 235, Loss: 0.6242875\n",
      "Epoch: 236, Loss: 0.6155735\n",
      "Epoch: 237, Loss: 0.5959700\n",
      "Epoch: 238, Loss: 0.5782411\n",
      "Epoch: 239, Loss: 0.5740687\n",
      "Epoch: 240, Loss: 0.5674237\n",
      "Epoch: 241, Loss: 0.5501760\n",
      "Epoch: 242, Loss: 0.5404550\n",
      "Epoch: 243, Loss: 0.5320165\n",
      "Epoch: 244, Loss: 0.5150596\n",
      "Epoch: 245, Loss: 0.5032682\n",
      "Epoch: 246, Loss: 0.5017731\n",
      "Epoch: 247, Loss: 0.4946698\n",
      "Epoch: 248, Loss: 0.4844870\n",
      "Epoch: 249, Loss: 0.4783532\n",
      "Epoch: 250, Loss: 0.4693160\n",
      "Epoch: 251, Loss: 0.4664549\n",
      "Epoch: 252, Loss: 0.4608019\n",
      "Epoch: 253, Loss: 0.4563661\n",
      "Epoch: 254, Loss: 0.4509773\n",
      "Epoch: 255, Loss: 0.4429453\n",
      "Epoch: 256, Loss: 0.4382167\n",
      "Epoch: 257, Loss: 0.4325461\n",
      "Epoch: 258, Loss: 0.4288150\n",
      "Epoch: 259, Loss: 0.4259228\n",
      "Epoch: 260, Loss: 0.4203907\n",
      "Epoch: 261, Loss: 0.4166411\n",
      "Epoch: 262, Loss: 0.4125725\n",
      "Epoch: 263, Loss: 0.4084764\n",
      "Epoch: 264, Loss: 0.4044746\n",
      "Epoch: 265, Loss: 0.4002125\n",
      "Epoch: 266, Loss: 0.3978005\n",
      "Epoch: 267, Loss: 0.3932988\n",
      "Epoch: 268, Loss: 0.3885969\n",
      "Epoch: 269, Loss: 0.3852116\n",
      "Epoch: 270, Loss: 0.3826011\n",
      "Epoch: 271, Loss: 0.3814102\n",
      "Epoch: 272, Loss: 0.3868283\n",
      "Epoch: 273, Loss: 0.3842542\n",
      "Epoch: 274, Loss: 0.3931806\n",
      "Epoch: 275, Loss: 0.3754202\n",
      "Epoch: 276, Loss: 0.3717611\n",
      "Epoch: 277, Loss: 0.3767294\n",
      "Epoch: 278, Loss: 0.3641462\n",
      "Epoch: 279, Loss: 0.3567864\n",
      "Epoch: 280, Loss: 0.3513778\n",
      "Epoch: 281, Loss: 0.3544123\n",
      "Epoch: 282, Loss: 0.3645557\n",
      "Epoch: 283, Loss: 0.3626398\n",
      "Epoch: 284, Loss: 0.3634067\n",
      "Epoch: 285, Loss: 0.3393597\n",
      "Epoch: 286, Loss: 0.3523972\n",
      "Epoch: 287, Loss: 0.4009502\n",
      "Epoch: 288, Loss: 0.3563726\n",
      "Epoch: 289, Loss: 0.4143218\n",
      "Epoch: 290, Loss: 0.4162798\n",
      "Epoch: 291, Loss: 0.4178936\n",
      "Epoch: 292, Loss: 0.3998208\n",
      "Epoch: 293, Loss: 0.3805063\n",
      "Epoch: 294, Loss: 0.3755034\n",
      "Epoch: 295, Loss: 0.3799674\n",
      "Epoch: 296, Loss: 0.3576508\n",
      "Epoch: 297, Loss: 0.3651350\n",
      "Epoch: 298, Loss: 0.3491431\n",
      "Epoch: 299, Loss: 0.3442319\n",
      "Epoch: 300, Loss: 0.3435764\n",
      "Epoch: 301, Loss: 0.3417556\n",
      "Epoch: 302, Loss: 0.3291709\n",
      "Epoch: 303, Loss: 0.3271379\n",
      "Epoch: 304, Loss: 0.3223474\n",
      "Epoch: 305, Loss: 0.3187227\n",
      "Epoch: 306, Loss: 0.3152732\n",
      "Epoch: 307, Loss: 0.3104624\n",
      "Epoch: 308, Loss: 0.3069746\n",
      "Epoch: 309, Loss: 0.3026038\n",
      "Epoch: 310, Loss: 0.3016578\n",
      "Epoch: 311, Loss: 0.2972513\n",
      "Epoch: 312, Loss: 0.2929830\n",
      "Epoch: 313, Loss: 0.2913319\n",
      "Epoch: 314, Loss: 0.2916592\n",
      "Epoch: 315, Loss: 0.3009848\n",
      "Epoch: 316, Loss: 0.3034715\n",
      "Epoch: 317, Loss: 0.3389120\n",
      "Epoch: 318, Loss: 0.2973823\n",
      "Epoch: 319, Loss: 0.3259678\n",
      "Epoch: 320, Loss: 0.3266476\n",
      "Epoch: 321, Loss: 0.3132615\n",
      "Epoch: 322, Loss: 0.3384767\n",
      "Epoch: 323, Loss: 0.3706336\n",
      "Epoch: 324, Loss: 0.3464753\n",
      "Epoch: 325, Loss: 0.3875189\n",
      "Epoch: 326, Loss: 0.3450180\n",
      "Epoch: 327, Loss: 0.3552601\n",
      "Epoch: 328, Loss: 0.3327464\n",
      "Epoch: 329, Loss: 0.3379319\n",
      "Epoch: 330, Loss: 0.3264551\n",
      "Epoch: 331, Loss: 0.3249113\n",
      "Epoch: 332, Loss: 0.3090604\n",
      "Epoch: 333, Loss: 0.3103156\n",
      "Epoch: 334, Loss: 0.3016542\n",
      "Epoch: 335, Loss: 0.2967587\n",
      "Epoch: 336, Loss: 0.2949354\n",
      "Epoch: 337, Loss: 0.2874780\n",
      "Epoch: 338, Loss: 0.2818235\n",
      "Epoch: 339, Loss: 0.2787367\n",
      "Epoch: 340, Loss: 0.2731400\n",
      "Epoch: 341, Loss: 0.2700786\n",
      "Epoch: 342, Loss: 0.2667518\n",
      "Epoch: 343, Loss: 0.2639524\n",
      "Epoch: 344, Loss: 0.2601259\n",
      "Epoch: 345, Loss: 0.2549495\n",
      "Epoch: 346, Loss: 0.2533125\n",
      "Epoch: 347, Loss: 0.2495890\n",
      "Epoch: 348, Loss: 0.2468161\n",
      "Epoch: 349, Loss: 0.2426573\n",
      "Epoch: 350, Loss: 0.2407805\n",
      "Epoch: 351, Loss: 0.2400116\n",
      "Epoch: 352, Loss: 0.2374676\n",
      "Epoch: 353, Loss: 0.2333533\n",
      "Epoch: 354, Loss: 0.2290900\n",
      "Epoch: 355, Loss: 0.2257773\n",
      "Epoch: 356, Loss: 0.2253287\n",
      "Epoch: 357, Loss: 0.2289203\n",
      "Epoch: 358, Loss: 0.2264644\n",
      "Epoch: 359, Loss: 0.2239625\n",
      "Epoch: 360, Loss: 0.2216006\n",
      "Epoch: 361, Loss: 0.2318829\n",
      "Epoch: 362, Loss: 0.2304982\n",
      "Epoch: 363, Loss: 0.2243635\n",
      "Epoch: 364, Loss: 0.2091523\n",
      "Epoch: 365, Loss: 0.2114867\n",
      "Epoch: 366, Loss: 0.2190312\n",
      "Epoch: 367, Loss: 0.2201093\n",
      "Epoch: 368, Loss: 0.2219460\n",
      "Epoch: 369, Loss: 0.2201725\n",
      "Epoch: 370, Loss: 0.2277842\n",
      "Epoch: 371, Loss: 0.2052251\n",
      "Epoch: 372, Loss: 0.2140952\n",
      "Epoch: 373, Loss: 0.2174177\n",
      "Epoch: 374, Loss: 0.2017848\n",
      "Epoch: 375, Loss: 0.2218757\n",
      "Epoch: 376, Loss: 0.2052222\n",
      "Epoch: 377, Loss: 0.2080882\n",
      "Epoch: 378, Loss: 0.2023707\n",
      "Epoch: 379, Loss: 0.1916635\n",
      "Epoch: 380, Loss: 0.1953680\n",
      "Epoch: 381, Loss: 0.1884440\n",
      "Epoch: 382, Loss: 0.1930161\n",
      "Epoch: 383, Loss: 0.1870338\n",
      "Epoch: 384, Loss: 0.1914140\n",
      "Epoch: 385, Loss: 0.1892822\n",
      "Epoch: 386, Loss: 0.2018640\n",
      "Epoch: 387, Loss: 0.1951492\n",
      "Epoch: 388, Loss: 0.2042028\n",
      "Epoch: 389, Loss: 0.1945870\n",
      "Epoch: 390, Loss: 0.1867764\n",
      "Epoch: 391, Loss: 0.1945326\n",
      "Epoch: 392, Loss: 0.1919048\n",
      "Epoch: 393, Loss: 0.1989333\n",
      "Epoch: 394, Loss: 0.1904131\n",
      "Epoch: 395, Loss: 0.1778259\n",
      "Epoch: 396, Loss: 0.1752783\n",
      "Epoch: 397, Loss: 0.1799764\n",
      "Epoch: 398, Loss: 0.1905417\n",
      "Epoch: 399, Loss: 0.1850342\n",
      "Epoch: 400, Loss: 0.1712241\n",
      "Epoch: 401, Loss: 0.1750859\n",
      "Epoch: 402, Loss: 0.1652353\n",
      "Epoch: 403, Loss: 0.1635667\n",
      "Epoch: 404, Loss: 0.1640998\n",
      "Epoch: 405, Loss: 0.1556428\n",
      "Epoch: 406, Loss: 0.1568682\n",
      "Epoch: 407, Loss: 0.1556840\n",
      "Epoch: 408, Loss: 0.1480795\n",
      "Epoch: 409, Loss: 0.1519339\n",
      "Epoch: 410, Loss: 0.1485206\n",
      "Epoch: 411, Loss: 0.1451419\n",
      "Epoch: 412, Loss: 0.1456184\n",
      "Epoch: 413, Loss: 0.1423031\n",
      "Epoch: 414, Loss: 0.1452820\n",
      "Epoch: 415, Loss: 0.1530642\n",
      "Epoch: 416, Loss: 0.1473492\n",
      "Epoch: 417, Loss: 0.1617218\n",
      "Epoch: 418, Loss: 0.1588836\n",
      "Epoch: 419, Loss: 0.1614416\n",
      "Epoch: 420, Loss: 0.1487685\n",
      "Epoch: 421, Loss: 0.1532880\n",
      "Epoch: 422, Loss: 0.1631510\n",
      "Epoch: 423, Loss: 0.1829918\n",
      "Epoch: 424, Loss: 0.2235807\n",
      "Epoch: 425, Loss: 0.1893702\n",
      "Epoch: 426, Loss: 0.1702171\n",
      "Epoch: 427, Loss: 0.1623689\n",
      "Epoch: 428, Loss: 0.1656345\n",
      "Epoch: 429, Loss: 0.1534311\n",
      "Epoch: 430, Loss: 0.1530600\n",
      "Epoch: 431, Loss: 0.1446345\n",
      "Epoch: 432, Loss: 0.1420090\n",
      "Epoch: 433, Loss: 0.1425375\n",
      "Epoch: 434, Loss: 0.1359513\n",
      "Epoch: 435, Loss: 0.1339232\n",
      "Epoch: 436, Loss: 0.1334530\n",
      "Epoch: 437, Loss: 0.1256298\n",
      "Epoch: 438, Loss: 0.1277868\n",
      "Epoch: 439, Loss: 0.1251074\n",
      "Epoch: 440, Loss: 0.1196711\n",
      "Epoch: 441, Loss: 0.1202953\n",
      "Epoch: 442, Loss: 0.1189205\n",
      "Epoch: 443, Loss: 0.1158411\n",
      "Epoch: 444, Loss: 0.1150063\n",
      "Epoch: 445, Loss: 0.1130360\n",
      "Epoch: 446, Loss: 0.1095324\n",
      "Epoch: 447, Loss: 0.1073600\n",
      "Epoch: 448, Loss: 0.1073121\n",
      "Epoch: 449, Loss: 0.1062651\n",
      "Epoch: 450, Loss: 0.1065387\n",
      "Epoch: 451, Loss: 0.1101483\n",
      "Epoch: 452, Loss: 0.1114166\n",
      "Epoch: 453, Loss: 0.1173129\n",
      "Epoch: 454, Loss: 0.1136402\n",
      "Epoch: 455, Loss: 0.1115122\n",
      "Epoch: 456, Loss: 0.0989894\n",
      "Epoch: 457, Loss: 0.1001180\n",
      "Epoch: 458, Loss: 0.1089787\n",
      "Epoch: 459, Loss: 0.1107112\n",
      "Epoch: 460, Loss: 0.1168968\n",
      "Epoch: 461, Loss: 0.1063342\n",
      "Epoch: 462, Loss: 0.1020177\n",
      "Epoch: 463, Loss: 0.1020373\n",
      "Epoch: 464, Loss: 0.1062733\n",
      "Epoch: 465, Loss: 0.1014659\n",
      "Epoch: 466, Loss: 0.0902774\n",
      "Epoch: 467, Loss: 0.0955835\n",
      "Epoch: 468, Loss: 0.0963674\n",
      "Epoch: 469, Loss: 0.0972310\n",
      "Epoch: 470, Loss: 0.0958587\n",
      "Epoch: 471, Loss: 0.0879310\n",
      "Epoch: 472, Loss: 0.0888350\n",
      "Epoch: 473, Loss: 0.0948929\n",
      "Epoch: 474, Loss: 0.0927975\n",
      "Epoch: 475, Loss: 0.0872584\n",
      "Epoch: 476, Loss: 0.0821986\n",
      "Epoch: 477, Loss: 0.0864651\n",
      "Epoch: 478, Loss: 0.0940311\n",
      "Epoch: 479, Loss: 0.0885216\n",
      "Epoch: 480, Loss: 0.0969851\n",
      "Epoch: 481, Loss: 0.0889747\n",
      "Epoch: 482, Loss: 0.0886867\n",
      "Epoch: 483, Loss: 0.0894221\n",
      "Epoch: 484, Loss: 0.0903038\n",
      "Epoch: 485, Loss: 0.0825600\n",
      "Epoch: 486, Loss: 0.0832619\n",
      "Epoch: 487, Loss: 0.0800408\n",
      "Epoch: 488, Loss: 0.0786385\n",
      "Epoch: 489, Loss: 0.0765638\n",
      "Epoch: 490, Loss: 0.0802265\n",
      "Epoch: 491, Loss: 0.0741591\n",
      "Epoch: 492, Loss: 0.0737240\n",
      "Epoch: 493, Loss: 0.0746513\n",
      "Epoch: 494, Loss: 0.0782632\n",
      "Epoch: 495, Loss: 0.0746972\n",
      "Epoch: 496, Loss: 0.0761373\n",
      "Epoch: 497, Loss: 0.0744209\n",
      "Epoch: 498, Loss: 0.0694645\n",
      "Epoch: 499, Loss: 0.0734299\n",
      "Epoch: 500, Loss: 0.0736879\n",
      "Epoch: 501, Loss: 0.0767440\n",
      "Epoch: 502, Loss: 0.0730977\n",
      "Epoch: 503, Loss: 0.0709750\n",
      "Epoch: 504, Loss: 0.0743661\n",
      "Epoch: 505, Loss: 0.0721569\n",
      "Epoch: 506, Loss: 0.0739355\n",
      "Epoch: 507, Loss: 0.0728698\n",
      "Epoch: 508, Loss: 0.0657969\n",
      "Epoch: 509, Loss: 0.0674859\n",
      "Epoch: 510, Loss: 0.0670073\n",
      "Epoch: 511, Loss: 0.0650176\n",
      "Epoch: 512, Loss: 0.0587082\n",
      "Epoch: 513, Loss: 0.0654169\n",
      "Epoch: 514, Loss: 0.0613502\n",
      "Epoch: 515, Loss: 0.0646447\n",
      "Epoch: 516, Loss: 0.0676880\n",
      "Epoch: 517, Loss: 0.0746071\n",
      "Epoch: 518, Loss: 0.0715749\n",
      "Epoch: 519, Loss: 0.0716220\n",
      "Epoch: 520, Loss: 0.0670448\n",
      "Epoch: 521, Loss: 0.0623373\n",
      "Epoch: 522, Loss: 0.0567436\n",
      "Epoch: 523, Loss: 0.0596072\n",
      "Epoch: 524, Loss: 0.0582342\n",
      "Epoch: 525, Loss: 0.0602554\n",
      "Epoch: 526, Loss: 0.0536796\n",
      "Epoch: 527, Loss: 0.0534332\n",
      "Epoch: 528, Loss: 0.0514806\n",
      "Epoch: 529, Loss: 0.0532643\n",
      "Epoch: 530, Loss: 0.0514222\n",
      "Epoch: 531, Loss: 0.0531446\n",
      "Epoch: 532, Loss: 0.0489979\n",
      "Epoch: 533, Loss: 0.0509904\n",
      "Epoch: 534, Loss: 0.0540554\n",
      "Epoch: 535, Loss: 0.0600262\n",
      "Epoch: 536, Loss: 0.0619222\n",
      "Epoch: 537, Loss: 0.0535358\n",
      "Epoch: 538, Loss: 0.0484361\n",
      "Epoch: 539, Loss: 0.0524331\n",
      "Epoch: 540, Loss: 0.0540356\n",
      "Epoch: 541, Loss: 0.0464568\n",
      "Epoch: 542, Loss: 0.0461720\n",
      "Epoch: 543, Loss: 0.0486274\n",
      "Epoch: 544, Loss: 0.0416966\n",
      "Epoch: 545, Loss: 0.0430204\n",
      "Epoch: 546, Loss: 0.0449721\n",
      "Epoch: 547, Loss: 0.0394967\n",
      "Epoch: 548, Loss: 0.0440546\n",
      "Epoch: 549, Loss: 0.0452761\n",
      "Epoch: 550, Loss: 0.0498593\n",
      "Epoch: 551, Loss: 0.0669747\n",
      "Epoch: 552, Loss: 0.0663247\n",
      "Epoch: 553, Loss: 0.0732164\n",
      "Epoch: 554, Loss: 0.0474722\n",
      "Epoch: 555, Loss: 0.0514360\n",
      "Epoch: 556, Loss: 0.0671113\n",
      "Epoch: 557, Loss: 0.0578085\n",
      "Epoch: 558, Loss: 0.0610503\n",
      "Epoch: 559, Loss: 0.0523508\n",
      "Epoch: 560, Loss: 0.0569904\n",
      "Epoch: 561, Loss: 0.0534353\n",
      "Epoch: 562, Loss: 0.0523727\n",
      "Epoch: 563, Loss: 0.0514713\n",
      "Epoch: 564, Loss: 0.0430262\n",
      "Epoch: 565, Loss: 0.0505983\n",
      "Epoch: 566, Loss: 0.0406568\n",
      "Epoch: 567, Loss: 0.0434494\n",
      "Epoch: 568, Loss: 0.0377612\n",
      "Epoch: 569, Loss: 0.0399423\n",
      "Epoch: 570, Loss: 0.0362930\n",
      "Epoch: 571, Loss: 0.0355868\n",
      "Epoch: 572, Loss: 0.0360607\n",
      "Epoch: 573, Loss: 0.0335115\n",
      "Epoch: 574, Loss: 0.0360582\n",
      "Epoch: 575, Loss: 0.0347153\n",
      "Epoch: 576, Loss: 0.0353173\n",
      "Epoch: 577, Loss: 0.0323544\n",
      "Epoch: 578, Loss: 0.0327004\n",
      "Epoch: 579, Loss: 0.0296347\n",
      "Epoch: 580, Loss: 0.0293715\n",
      "Epoch: 581, Loss: 0.0283438\n",
      "Epoch: 582, Loss: 0.0288538\n",
      "Epoch: 583, Loss: 0.0280909\n",
      "Epoch: 584, Loss: 0.0285577\n",
      "Epoch: 585, Loss: 0.0280384\n",
      "Epoch: 586, Loss: 0.0286142\n",
      "Epoch: 587, Loss: 0.0276384\n",
      "Epoch: 588, Loss: 0.0271428\n",
      "Epoch: 589, Loss: 0.0264586\n",
      "Epoch: 590, Loss: 0.0249993\n",
      "Epoch: 591, Loss: 0.0238696\n",
      "Epoch: 592, Loss: 0.0228980\n",
      "Epoch: 593, Loss: 0.0232719\n",
      "Epoch: 594, Loss: 0.0228178\n",
      "Epoch: 595, Loss: 0.0227999\n",
      "Epoch: 596, Loss: 0.0227514\n",
      "Epoch: 597, Loss: 0.0223452\n",
      "Epoch: 598, Loss: 0.0224394\n",
      "Epoch: 599, Loss: 0.0237896\n",
      "Epoch: 600, Loss: 0.0275009\n",
      "Epoch: 601, Loss: 0.0402043\n",
      "Epoch: 602, Loss: 0.0541991\n",
      "Epoch: 603, Loss: 0.0755646\n",
      "Epoch: 604, Loss: 0.0396977\n",
      "Epoch: 605, Loss: 0.0503478\n",
      "Epoch: 606, Loss: 0.0594629\n",
      "Epoch: 607, Loss: 0.0335757\n",
      "Epoch: 608, Loss: 0.0489138\n",
      "Epoch: 609, Loss: 0.0409583\n",
      "Epoch: 610, Loss: 0.0371386\n",
      "Epoch: 611, Loss: 0.0374662\n",
      "Epoch: 612, Loss: 0.0381626\n",
      "Epoch: 613, Loss: 0.0340406\n",
      "Epoch: 614, Loss: 0.0352786\n",
      "Epoch: 615, Loss: 0.0280362\n",
      "Epoch: 616, Loss: 0.0341926\n",
      "Epoch: 617, Loss: 0.0275231\n",
      "Epoch: 618, Loss: 0.0312941\n",
      "Epoch: 619, Loss: 0.0268764\n",
      "Epoch: 620, Loss: 0.0286166\n",
      "Epoch: 621, Loss: 0.0257403\n",
      "Epoch: 622, Loss: 0.0247473\n",
      "Epoch: 623, Loss: 0.0243821\n",
      "Epoch: 624, Loss: 0.0215178\n",
      "Epoch: 625, Loss: 0.0230297\n",
      "Epoch: 626, Loss: 0.0197508\n",
      "Epoch: 627, Loss: 0.0207706\n",
      "Epoch: 628, Loss: 0.0192653\n",
      "Epoch: 629, Loss: 0.0197163\n",
      "Epoch: 630, Loss: 0.0185998\n",
      "Epoch: 631, Loss: 0.0179023\n",
      "Epoch: 632, Loss: 0.0177676\n",
      "Epoch: 633, Loss: 0.0172577\n",
      "Epoch: 634, Loss: 0.0175931\n",
      "Epoch: 635, Loss: 0.0163547\n",
      "Epoch: 636, Loss: 0.0175852\n",
      "Epoch: 637, Loss: 0.0170767\n",
      "Epoch: 638, Loss: 0.0185270\n",
      "Epoch: 639, Loss: 0.0195612\n",
      "Epoch: 640, Loss: 0.0244045\n",
      "Epoch: 641, Loss: 0.0288171\n",
      "Epoch: 642, Loss: 0.0330382\n",
      "Epoch: 643, Loss: 0.0301738\n",
      "Epoch: 644, Loss: 0.0269819\n",
      "Epoch: 645, Loss: 0.0250068\n",
      "Epoch: 646, Loss: 0.0326513\n",
      "Epoch: 647, Loss: 0.0472783\n",
      "Epoch: 648, Loss: 0.0355558\n",
      "Epoch: 649, Loss: 0.0272863\n",
      "Epoch: 650, Loss: 0.0221792\n",
      "Epoch: 651, Loss: 0.0297057\n",
      "Epoch: 652, Loss: 0.0313813\n",
      "Epoch: 653, Loss: 0.0212420\n",
      "Epoch: 654, Loss: 0.0250855\n",
      "Epoch: 655, Loss: 0.0283963\n",
      "Epoch: 656, Loss: 0.0200751\n",
      "Epoch: 657, Loss: 0.0222903\n",
      "Epoch: 658, Loss: 0.0204131\n",
      "Epoch: 659, Loss: 0.0190103\n",
      "Epoch: 660, Loss: 0.0203786\n",
      "Epoch: 661, Loss: 0.0158994\n",
      "Epoch: 662, Loss: 0.0189304\n",
      "Epoch: 663, Loss: 0.0155674\n",
      "Epoch: 664, Loss: 0.0160387\n",
      "Epoch: 665, Loss: 0.0160008\n",
      "Epoch: 666, Loss: 0.0136719\n",
      "Epoch: 667, Loss: 0.0155214\n",
      "Epoch: 668, Loss: 0.0130334\n",
      "Epoch: 669, Loss: 0.0151130\n",
      "Epoch: 670, Loss: 0.0116941\n",
      "Epoch: 671, Loss: 0.0137716\n",
      "Epoch: 672, Loss: 0.0115286\n",
      "Epoch: 673, Loss: 0.0123145\n",
      "Epoch: 674, Loss: 0.0120207\n",
      "Epoch: 675, Loss: 0.0111858\n",
      "Epoch: 676, Loss: 0.0112260\n",
      "Epoch: 677, Loss: 0.0102158\n",
      "Epoch: 678, Loss: 0.0108988\n",
      "Epoch: 679, Loss: 0.0102073\n",
      "Epoch: 680, Loss: 0.0104429\n",
      "Epoch: 681, Loss: 0.0096616\n",
      "Epoch: 682, Loss: 0.0099014\n",
      "Epoch: 683, Loss: 0.0094141\n",
      "Epoch: 684, Loss: 0.0095115\n",
      "Epoch: 685, Loss: 0.0093477\n",
      "Epoch: 686, Loss: 0.0093032\n",
      "Epoch: 687, Loss: 0.0093805\n",
      "Epoch: 688, Loss: 0.0091940\n",
      "Epoch: 689, Loss: 0.0096029\n",
      "Epoch: 690, Loss: 0.0097271\n",
      "Epoch: 691, Loss: 0.0102523\n",
      "Epoch: 692, Loss: 0.0112294\n",
      "Epoch: 693, Loss: 0.0118411\n",
      "Epoch: 694, Loss: 0.0124064\n",
      "Epoch: 695, Loss: 0.0123788\n",
      "Epoch: 696, Loss: 0.0126433\n",
      "Epoch: 697, Loss: 0.0145956\n",
      "Epoch: 698, Loss: 0.0196110\n",
      "Epoch: 699, Loss: 0.0239269\n",
      "Epoch: 700, Loss: 0.0232161\n",
      "Epoch: 701, Loss: 0.0134926\n",
      "Epoch: 702, Loss: 0.0115150\n",
      "Epoch: 703, Loss: 0.0156876\n",
      "Epoch: 704, Loss: 0.0145301\n",
      "Epoch: 705, Loss: 0.0128894\n",
      "Epoch: 706, Loss: 0.0167126\n",
      "Epoch: 707, Loss: 0.0157598\n",
      "Epoch: 708, Loss: 0.0112803\n",
      "Epoch: 709, Loss: 0.0118401\n",
      "Epoch: 710, Loss: 0.0109034\n",
      "Epoch: 711, Loss: 0.0091546\n",
      "Epoch: 712, Loss: 0.0107518\n",
      "Epoch: 713, Loss: 0.0104793\n",
      "Epoch: 714, Loss: 0.0097784\n",
      "Epoch: 715, Loss: 0.0090773\n",
      "Epoch: 716, Loss: 0.0079496\n",
      "Epoch: 717, Loss: 0.0083534\n",
      "Epoch: 718, Loss: 0.0085609\n",
      "Epoch: 719, Loss: 0.0082467\n",
      "Epoch: 720, Loss: 0.0089290\n",
      "Epoch: 721, Loss: 0.0084180\n",
      "Epoch: 722, Loss: 0.0084142\n",
      "Epoch: 723, Loss: 0.0086374\n",
      "Epoch: 724, Loss: 0.0077719\n",
      "Epoch: 725, Loss: 0.0076580\n",
      "Epoch: 726, Loss: 0.0072626\n",
      "Epoch: 727, Loss: 0.0064302\n",
      "Epoch: 728, Loss: 0.0068645\n",
      "Epoch: 729, Loss: 0.0066781\n",
      "Epoch: 730, Loss: 0.0064073\n",
      "Epoch: 731, Loss: 0.0070974\n",
      "Epoch: 732, Loss: 0.0071652\n",
      "Epoch: 733, Loss: 0.0071846\n",
      "Epoch: 734, Loss: 0.0081052\n",
      "Epoch: 735, Loss: 0.0091738\n",
      "Epoch: 736, Loss: 0.0114237\n",
      "Epoch: 737, Loss: 0.0145643\n",
      "Epoch: 738, Loss: 0.0159878\n",
      "Epoch: 739, Loss: 0.0129511\n",
      "Epoch: 740, Loss: 0.0097361\n",
      "Epoch: 741, Loss: 0.0094676\n",
      "Epoch: 742, Loss: 0.0133620\n",
      "Epoch: 743, Loss: 0.0174661\n",
      "Epoch: 744, Loss: 0.0190621\n",
      "Epoch: 745, Loss: 0.0223372\n",
      "Epoch: 746, Loss: 0.0249697\n",
      "Epoch: 747, Loss: 0.0171850\n",
      "Epoch: 748, Loss: 0.0120758\n",
      "Epoch: 749, Loss: 0.0155275\n",
      "Epoch: 750, Loss: 0.0135170\n",
      "Epoch: 751, Loss: 0.0131165\n",
      "Epoch: 752, Loss: 0.0157706\n",
      "Epoch: 753, Loss: 0.0138950\n",
      "Epoch: 754, Loss: 0.0151301\n",
      "Epoch: 755, Loss: 0.0145198\n",
      "Epoch: 756, Loss: 0.0123717\n",
      "Epoch: 757, Loss: 0.0121244\n",
      "Epoch: 758, Loss: 0.0091472\n",
      "Epoch: 759, Loss: 0.0093428\n",
      "Epoch: 760, Loss: 0.0078339\n",
      "Epoch: 761, Loss: 0.0078905\n",
      "Epoch: 762, Loss: 0.0095109\n",
      "Epoch: 763, Loss: 0.0078874\n",
      "Epoch: 764, Loss: 0.0091081\n",
      "Epoch: 765, Loss: 0.0073896\n",
      "Epoch: 766, Loss: 0.0070332\n",
      "Epoch: 767, Loss: 0.0063575\n",
      "Epoch: 768, Loss: 0.0054256\n",
      "Epoch: 769, Loss: 0.0060807\n",
      "Epoch: 770, Loss: 0.0056462\n",
      "Epoch: 771, Loss: 0.0060440\n",
      "Epoch: 772, Loss: 0.0056988\n",
      "Epoch: 773, Loss: 0.0054010\n",
      "Epoch: 774, Loss: 0.0052811\n",
      "Epoch: 775, Loss: 0.0049322\n",
      "Epoch: 776, Loss: 0.0051446\n",
      "Epoch: 777, Loss: 0.0047450\n",
      "Epoch: 778, Loss: 0.0045828\n",
      "Epoch: 779, Loss: 0.0042935\n",
      "Epoch: 780, Loss: 0.0041368\n",
      "Epoch: 781, Loss: 0.0041631\n",
      "Epoch: 782, Loss: 0.0039606\n",
      "Epoch: 783, Loss: 0.0040247\n",
      "Epoch: 784, Loss: 0.0039815\n",
      "Epoch: 785, Loss: 0.0038579\n",
      "Epoch: 786, Loss: 0.0039422\n",
      "Epoch: 787, Loss: 0.0038149\n",
      "Epoch: 788, Loss: 0.0039848\n",
      "Epoch: 789, Loss: 0.0042419\n",
      "Epoch: 790, Loss: 0.0047810\n",
      "Epoch: 791, Loss: 0.0061117\n",
      "Epoch: 792, Loss: 0.0087759\n",
      "Epoch: 793, Loss: 0.0129879\n",
      "Epoch: 794, Loss: 0.0230072\n",
      "Epoch: 795, Loss: 0.0300803\n",
      "Epoch: 796, Loss: 0.0398688\n",
      "Epoch: 797, Loss: 0.0221930\n",
      "Epoch: 798, Loss: 0.0106199\n",
      "Epoch: 799, Loss: 0.0146958\n",
      "Epoch: 800, Loss: 0.0198488\n",
      "Epoch: 801, Loss: 0.0187526\n",
      "Epoch: 802, Loss: 0.0152487\n",
      "Epoch: 803, Loss: 0.0170184\n",
      "Epoch: 804, Loss: 0.0176351\n",
      "Epoch: 805, Loss: 0.0143358\n",
      "Epoch: 806, Loss: 0.0127248\n",
      "Epoch: 807, Loss: 0.0148916\n",
      "Epoch: 808, Loss: 0.0098955\n",
      "Epoch: 809, Loss: 0.0109091\n",
      "Epoch: 810, Loss: 0.0148770\n",
      "Epoch: 811, Loss: 0.0087920\n",
      "Epoch: 812, Loss: 0.0097149\n",
      "Epoch: 813, Loss: 0.0096929\n",
      "Epoch: 814, Loss: 0.0074528\n",
      "Epoch: 815, Loss: 0.0087961\n",
      "Epoch: 816, Loss: 0.0075611\n",
      "Epoch: 817, Loss: 0.0070007\n",
      "Epoch: 818, Loss: 0.0074354\n",
      "Epoch: 819, Loss: 0.0059988\n",
      "Epoch: 820, Loss: 0.0059558\n",
      "Epoch: 821, Loss: 0.0060390\n",
      "Epoch: 822, Loss: 0.0052847\n",
      "Epoch: 823, Loss: 0.0056101\n",
      "Epoch: 824, Loss: 0.0046804\n",
      "Epoch: 825, Loss: 0.0048251\n",
      "Epoch: 826, Loss: 0.0047553\n",
      "Epoch: 827, Loss: 0.0043915\n",
      "Epoch: 828, Loss: 0.0045845\n",
      "Epoch: 829, Loss: 0.0036818\n",
      "Epoch: 830, Loss: 0.0041262\n",
      "Epoch: 831, Loss: 0.0036562\n",
      "Epoch: 832, Loss: 0.0038999\n",
      "Epoch: 833, Loss: 0.0037156\n",
      "Epoch: 834, Loss: 0.0034492\n",
      "Epoch: 835, Loss: 0.0034232\n",
      "Epoch: 836, Loss: 0.0032374\n",
      "Epoch: 837, Loss: 0.0031753\n",
      "Epoch: 838, Loss: 0.0031172\n",
      "Epoch: 839, Loss: 0.0031211\n",
      "Epoch: 840, Loss: 0.0029686\n",
      "Epoch: 841, Loss: 0.0029551\n",
      "Epoch: 842, Loss: 0.0026854\n",
      "Epoch: 843, Loss: 0.0027185\n",
      "Epoch: 844, Loss: 0.0025817\n",
      "Epoch: 845, Loss: 0.0026011\n",
      "Epoch: 846, Loss: 0.0025854\n",
      "Epoch: 847, Loss: 0.0025455\n",
      "Epoch: 848, Loss: 0.0025466\n",
      "Epoch: 849, Loss: 0.0026061\n",
      "Epoch: 850, Loss: 0.0026952\n",
      "Epoch: 851, Loss: 0.0030105\n",
      "Epoch: 852, Loss: 0.0036494\n",
      "Epoch: 853, Loss: 0.0051546\n",
      "Epoch: 854, Loss: 0.0081569\n",
      "Epoch: 855, Loss: 0.0142709\n",
      "Epoch: 856, Loss: 0.0231948\n",
      "Epoch: 857, Loss: 0.0304128\n",
      "Epoch: 858, Loss: 0.0242284\n",
      "Epoch: 859, Loss: 0.0117266\n",
      "Epoch: 860, Loss: 0.0175832\n",
      "Epoch: 861, Loss: 0.0176405\n",
      "Epoch: 862, Loss: 0.0106382\n",
      "Epoch: 863, Loss: 0.0155606\n",
      "Epoch: 864, Loss: 0.0100436\n",
      "Epoch: 865, Loss: 0.0109509\n",
      "Epoch: 866, Loss: 0.0120987\n",
      "Epoch: 867, Loss: 0.0084661\n",
      "Epoch: 868, Loss: 0.0106404\n",
      "Epoch: 869, Loss: 0.0059748\n",
      "Epoch: 870, Loss: 0.0079747\n",
      "Epoch: 871, Loss: 0.0067305\n",
      "Epoch: 872, Loss: 0.0074650\n",
      "Epoch: 873, Loss: 0.0067824\n",
      "Epoch: 874, Loss: 0.0062591\n",
      "Epoch: 875, Loss: 0.0068558\n",
      "Epoch: 876, Loss: 0.0053527\n",
      "Epoch: 877, Loss: 0.0051818\n",
      "Epoch: 878, Loss: 0.0043059\n",
      "Epoch: 879, Loss: 0.0048539\n",
      "Epoch: 880, Loss: 0.0039586\n",
      "Epoch: 881, Loss: 0.0041321\n",
      "Epoch: 882, Loss: 0.0037506\n",
      "Epoch: 883, Loss: 0.0039749\n",
      "Epoch: 884, Loss: 0.0036373\n",
      "Epoch: 885, Loss: 0.0036225\n",
      "Epoch: 886, Loss: 0.0033797\n",
      "Epoch: 887, Loss: 0.0034576\n",
      "Epoch: 888, Loss: 0.0032838\n",
      "Epoch: 889, Loss: 0.0030743\n",
      "Epoch: 890, Loss: 0.0029494\n",
      "Epoch: 891, Loss: 0.0030875\n",
      "Epoch: 892, Loss: 0.0029940\n",
      "Epoch: 893, Loss: 0.0028404\n",
      "Epoch: 894, Loss: 0.0027574\n",
      "Epoch: 895, Loss: 0.0028130\n",
      "Epoch: 896, Loss: 0.0029905\n",
      "Epoch: 897, Loss: 0.0030807\n",
      "Epoch: 898, Loss: 0.0033513\n",
      "Epoch: 899, Loss: 0.0036855\n",
      "Epoch: 900, Loss: 0.0045150\n",
      "Epoch: 901, Loss: 0.0052964\n",
      "Epoch: 902, Loss: 0.0066607\n",
      "Epoch: 903, Loss: 0.0076175\n",
      "Epoch: 904, Loss: 0.0091611\n",
      "Epoch: 905, Loss: 0.0086054\n",
      "Epoch: 906, Loss: 0.0077215\n",
      "Epoch: 907, Loss: 0.0052453\n",
      "Epoch: 908, Loss: 0.0038240\n",
      "Epoch: 909, Loss: 0.0031554\n",
      "Epoch: 910, Loss: 0.0034425\n",
      "Epoch: 911, Loss: 0.0041351\n",
      "Epoch: 912, Loss: 0.0040653\n",
      "Epoch: 913, Loss: 0.0039358\n",
      "Epoch: 914, Loss: 0.0035218\n",
      "Epoch: 915, Loss: 0.0030798\n",
      "Epoch: 916, Loss: 0.0024258\n",
      "Epoch: 917, Loss: 0.0022657\n",
      "Epoch: 918, Loss: 0.0025145\n",
      "Epoch: 919, Loss: 0.0027099\n",
      "Epoch: 920, Loss: 0.0026188\n",
      "Epoch: 921, Loss: 0.0022192\n",
      "Epoch: 922, Loss: 0.0020251\n",
      "Epoch: 923, Loss: 0.0021440\n",
      "Epoch: 924, Loss: 0.0022470\n",
      "Epoch: 925, Loss: 0.0021810\n",
      "Epoch: 926, Loss: 0.0018446\n",
      "Epoch: 927, Loss: 0.0016471\n",
      "Epoch: 928, Loss: 0.0017246\n",
      "Epoch: 929, Loss: 0.0018446\n",
      "Epoch: 930, Loss: 0.0018119\n",
      "Epoch: 931, Loss: 0.0016710\n",
      "Epoch: 932, Loss: 0.0016461\n",
      "Epoch: 933, Loss: 0.0017503\n",
      "Epoch: 934, Loss: 0.0020611\n",
      "Epoch: 935, Loss: 0.0025401\n",
      "Epoch: 936, Loss: 0.0032855\n",
      "Epoch: 937, Loss: 0.0044392\n",
      "Epoch: 938, Loss: 0.0060109\n",
      "Epoch: 939, Loss: 0.0079325\n",
      "Epoch: 940, Loss: 0.0088041\n",
      "Epoch: 941, Loss: 0.0076553\n",
      "Epoch: 942, Loss: 0.0048426\n",
      "Epoch: 943, Loss: 0.0039016\n",
      "Epoch: 944, Loss: 0.0057741\n",
      "Epoch: 945, Loss: 0.0061304\n",
      "Epoch: 946, Loss: 0.0035197\n",
      "Epoch: 947, Loss: 0.0031410\n",
      "Epoch: 948, Loss: 0.0049165\n",
      "Epoch: 949, Loss: 0.0048311\n",
      "Epoch: 950, Loss: 0.0039361\n",
      "Epoch: 951, Loss: 0.0069841\n",
      "Epoch: 952, Loss: 0.0101058\n",
      "Epoch: 953, Loss: 0.0148785\n",
      "Epoch: 954, Loss: 0.0226347\n",
      "Epoch: 955, Loss: 0.0372296\n",
      "Epoch: 956, Loss: 0.0322259\n",
      "Epoch: 957, Loss: 0.0325076\n",
      "Epoch: 958, Loss: 0.0279606\n",
      "Epoch: 959, Loss: 0.0245086\n",
      "Epoch: 960, Loss: 0.0293627\n",
      "Epoch: 961, Loss: 0.0387094\n",
      "Epoch: 962, Loss: 0.0491226\n",
      "Epoch: 963, Loss: 0.0508002\n",
      "Epoch: 964, Loss: 0.0415175\n",
      "Epoch: 965, Loss: 0.0258831\n",
      "Epoch: 966, Loss: 0.0394303\n",
      "Epoch: 967, Loss: 0.0294975\n",
      "Epoch: 968, Loss: 0.0396557\n",
      "Epoch: 969, Loss: 0.0375617\n",
      "Epoch: 970, Loss: 0.0321771\n",
      "Epoch: 971, Loss: 0.0334255\n",
      "Epoch: 972, Loss: 0.0294300\n",
      "Epoch: 973, Loss: 0.0298453\n",
      "Epoch: 974, Loss: 0.0209260\n",
      "Epoch: 975, Loss: 0.0256765\n",
      "Epoch: 976, Loss: 0.0167204\n",
      "Epoch: 977, Loss: 0.0207347\n",
      "Epoch: 978, Loss: 0.0166959\n",
      "Epoch: 979, Loss: 0.0141924\n",
      "Epoch: 980, Loss: 0.0167628\n",
      "Epoch: 981, Loss: 0.0103364\n",
      "Epoch: 982, Loss: 0.0136526\n",
      "Epoch: 983, Loss: 0.0111428\n",
      "Epoch: 984, Loss: 0.0087604\n",
      "Epoch: 985, Loss: 0.0112251\n",
      "Epoch: 986, Loss: 0.0074301\n",
      "Epoch: 987, Loss: 0.0081386\n",
      "Epoch: 988, Loss: 0.0077351\n",
      "Epoch: 989, Loss: 0.0063606\n",
      "Epoch: 990, Loss: 0.0064421\n",
      "Epoch: 991, Loss: 0.0062361\n",
      "Epoch: 992, Loss: 0.0049309\n",
      "Epoch: 993, Loss: 0.0056714\n",
      "Epoch: 994, Loss: 0.0046750\n",
      "Epoch: 995, Loss: 0.0045483\n",
      "Epoch: 996, Loss: 0.0042305\n",
      "Epoch: 997, Loss: 0.0039539\n",
      "Epoch: 998, Loss: 0.0036131\n",
      "Epoch: 999, Loss: 0.0037245\n",
      "Epoch: 1000, Loss: 0.0031096\n",
      "Epoch: 1001, Loss: 0.0031972\n",
      "Epoch: 1002, Loss: 0.0030156\n",
      "Epoch: 1003, Loss: 0.0027007\n",
      "Epoch: 1004, Loss: 0.0027114\n",
      "Epoch: 1005, Loss: 0.0025042\n",
      "Epoch: 1006, Loss: 0.0023922\n",
      "Epoch: 1007, Loss: 0.0023154\n",
      "Epoch: 1008, Loss: 0.0021660\n",
      "Epoch: 1009, Loss: 0.0020805\n",
      "Epoch: 1010, Loss: 0.0020266\n",
      "Epoch: 1011, Loss: 0.0019131\n",
      "Epoch: 1012, Loss: 0.0018713\n",
      "Epoch: 1013, Loss: 0.0017430\n",
      "Epoch: 1014, Loss: 0.0017358\n",
      "Epoch: 1015, Loss: 0.0016439\n",
      "Epoch: 1016, Loss: 0.0016124\n",
      "Epoch: 1017, Loss: 0.0015356\n",
      "Epoch: 1018, Loss: 0.0014957\n",
      "Epoch: 1019, Loss: 0.0014304\n",
      "Epoch: 1020, Loss: 0.0014241\n",
      "Epoch: 1021, Loss: 0.0013732\n",
      "Epoch: 1022, Loss: 0.0013208\n",
      "Epoch: 1023, Loss: 0.0013119\n",
      "Epoch: 1024, Loss: 0.0012453\n",
      "Epoch: 1025, Loss: 0.0012414\n",
      "Epoch: 1026, Loss: 0.0012114\n",
      "Epoch: 1027, Loss: 0.0011882\n",
      "Epoch: 1028, Loss: 0.0011659\n",
      "Epoch: 1029, Loss: 0.0011407\n",
      "Epoch: 1030, Loss: 0.0011028\n",
      "Epoch: 1031, Loss: 0.0010967\n",
      "Epoch: 1032, Loss: 0.0010570\n",
      "Epoch: 1033, Loss: 0.0010566\n",
      "Epoch: 1034, Loss: 0.0010368\n",
      "Epoch: 1035, Loss: 0.0010135\n",
      "Epoch: 1036, Loss: 0.0010088\n",
      "Epoch: 1037, Loss: 0.0009724\n",
      "Epoch: 1038, Loss: 0.0009704\n",
      "Epoch: 1039, Loss: 0.0009451\n",
      "Epoch: 1040, Loss: 0.0009375\n",
      "Epoch: 1041, Loss: 0.0009270\n",
      "Epoch: 1042, Loss: 0.0009082\n",
      "Epoch: 1043, Loss: 0.0009039\n",
      "Epoch: 1044, Loss: 0.0008872\n",
      "Epoch: 1045, Loss: 0.0008795\n",
      "Epoch: 1046, Loss: 0.0008682\n",
      "Epoch: 1047, Loss: 0.0008566\n",
      "Epoch: 1048, Loss: 0.0008500\n",
      "Epoch: 1049, Loss: 0.0008386\n",
      "Epoch: 1050, Loss: 0.0008337\n",
      "Epoch: 1051, Loss: 0.0008266\n",
      "Epoch: 1052, Loss: 0.0008224\n",
      "Epoch: 1053, Loss: 0.0008255\n",
      "Epoch: 1054, Loss: 0.0008306\n",
      "Epoch: 1055, Loss: 0.0008496\n",
      "Epoch: 1056, Loss: 0.0008770\n",
      "Epoch: 1057, Loss: 0.0009261\n",
      "Epoch: 1058, Loss: 0.0009990\n",
      "Epoch: 1059, Loss: 0.0011223\n",
      "Epoch: 1060, Loss: 0.0013036\n",
      "Epoch: 1061, Loss: 0.0016006\n",
      "Epoch: 1062, Loss: 0.0019911\n",
      "Epoch: 1063, Loss: 0.0026564\n",
      "Epoch: 1064, Loss: 0.0033695\n",
      "Epoch: 1065, Loss: 0.0045060\n",
      "Epoch: 1066, Loss: 0.0051586\n",
      "Epoch: 1067, Loss: 0.0060879\n",
      "Epoch: 1068, Loss: 0.0054224\n",
      "Epoch: 1069, Loss: 0.0045213\n",
      "Epoch: 1070, Loss: 0.0026285\n",
      "Epoch: 1071, Loss: 0.0012973\n",
      "Epoch: 1072, Loss: 0.0010332\n",
      "Epoch: 1073, Loss: 0.0017277\n",
      "Epoch: 1074, Loss: 0.0026910\n",
      "Epoch: 1075, Loss: 0.0029288\n",
      "Epoch: 1076, Loss: 0.0025872\n",
      "Epoch: 1077, Loss: 0.0016091\n",
      "Epoch: 1078, Loss: 0.0008737\n",
      "Epoch: 1079, Loss: 0.0008242\n",
      "Epoch: 1080, Loss: 0.0013066\n",
      "Epoch: 1081, Loss: 0.0017877\n",
      "Epoch: 1082, Loss: 0.0017801\n",
      "Epoch: 1083, Loss: 0.0014561\n",
      "Epoch: 1084, Loss: 0.0010167\n",
      "Epoch: 1085, Loss: 0.0007788\n",
      "Epoch: 1086, Loss: 0.0008005\n",
      "Epoch: 1087, Loss: 0.0009683\n",
      "Epoch: 1088, Loss: 0.0011365\n",
      "Epoch: 1089, Loss: 0.0011720\n",
      "Epoch: 1090, Loss: 0.0011356\n",
      "Epoch: 1091, Loss: 0.0010090\n",
      "Epoch: 1092, Loss: 0.0008801\n",
      "Epoch: 1093, Loss: 0.0007651\n",
      "Epoch: 1094, Loss: 0.0007095\n",
      "Epoch: 1095, Loss: 0.0007137\n",
      "Epoch: 1096, Loss: 0.0007553\n",
      "Epoch: 1097, Loss: 0.0008319\n",
      "Epoch: 1098, Loss: 0.0008928\n",
      "Epoch: 1099, Loss: 0.0009221\n",
      "Epoch: 1100, Loss: 0.0008864\n",
      "Epoch: 1101, Loss: 0.0008523\n",
      "Epoch: 1102, Loss: 0.0008253\n",
      "Epoch: 1103, Loss: 0.0008257\n",
      "Epoch: 1104, Loss: 0.0008219\n",
      "Epoch: 1105, Loss: 0.0008419\n",
      "Epoch: 1106, Loss: 0.0008918\n",
      "Epoch: 1107, Loss: 0.0010312\n",
      "Epoch: 1108, Loss: 0.0012800\n",
      "Epoch: 1109, Loss: 0.0016695\n",
      "Epoch: 1110, Loss: 0.0021678\n",
      "Epoch: 1111, Loss: 0.0026557\n",
      "Epoch: 1112, Loss: 0.0030797\n",
      "Epoch: 1113, Loss: 0.0030968\n",
      "Epoch: 1114, Loss: 0.0026945\n",
      "Epoch: 1115, Loss: 0.0019076\n",
      "Epoch: 1116, Loss: 0.0014972\n",
      "Epoch: 1117, Loss: 0.0019123\n",
      "Epoch: 1118, Loss: 0.0027320\n",
      "Epoch: 1119, Loss: 0.0032931\n",
      "Epoch: 1120, Loss: 0.0031256\n",
      "Epoch: 1121, Loss: 0.0027133\n",
      "Epoch: 1122, Loss: 0.0031476\n",
      "Epoch: 1123, Loss: 0.0049571\n",
      "Epoch: 1124, Loss: 0.0091338\n",
      "Epoch: 1125, Loss: 0.0131631\n",
      "Epoch: 1126, Loss: 0.0223893\n",
      "Epoch: 1127, Loss: 0.0232563\n",
      "Epoch: 1128, Loss: 0.0262769\n",
      "Epoch: 1129, Loss: 0.0237566\n",
      "Epoch: 1130, Loss: 0.0205328\n",
      "Epoch: 1131, Loss: 0.0143792\n",
      "Epoch: 1132, Loss: 0.0148213\n",
      "Epoch: 1133, Loss: 0.0154115\n",
      "Epoch: 1134, Loss: 0.0091602\n",
      "Epoch: 1135, Loss: 0.0093600\n",
      "Epoch: 1136, Loss: 0.0101012\n",
      "Epoch: 1137, Loss: 0.0060494\n",
      "Epoch: 1138, Loss: 0.0091642\n",
      "Epoch: 1139, Loss: 0.0056956\n",
      "Epoch: 1140, Loss: 0.0055338\n",
      "Epoch: 1141, Loss: 0.0072548\n",
      "Epoch: 1142, Loss: 0.0044679\n",
      "Epoch: 1143, Loss: 0.0065225\n",
      "Epoch: 1144, Loss: 0.0043869\n",
      "Epoch: 1145, Loss: 0.0050659\n",
      "Epoch: 1146, Loss: 0.0049014\n",
      "Epoch: 1147, Loss: 0.0035275\n",
      "Epoch: 1148, Loss: 0.0041981\n",
      "Epoch: 1149, Loss: 0.0031088\n",
      "Epoch: 1150, Loss: 0.0034590\n",
      "Epoch: 1151, Loss: 0.0031245\n",
      "Epoch: 1152, Loss: 0.0024934\n",
      "Epoch: 1153, Loss: 0.0025493\n",
      "Epoch: 1154, Loss: 0.0023812\n",
      "Epoch: 1155, Loss: 0.0022844\n",
      "Epoch: 1156, Loss: 0.0020066\n",
      "Epoch: 1157, Loss: 0.0018717\n",
      "Epoch: 1158, Loss: 0.0016242\n",
      "Epoch: 1159, Loss: 0.0016995\n",
      "Epoch: 1160, Loss: 0.0015629\n",
      "Epoch: 1161, Loss: 0.0014039\n",
      "Epoch: 1162, Loss: 0.0013830\n",
      "Epoch: 1163, Loss: 0.0012789\n",
      "Epoch: 1164, Loss: 0.0012327\n",
      "Epoch: 1165, Loss: 0.0011345\n",
      "Epoch: 1166, Loss: 0.0010355\n",
      "Epoch: 1167, Loss: 0.0009965\n",
      "Epoch: 1168, Loss: 0.0009451\n",
      "Epoch: 1169, Loss: 0.0009565\n",
      "Epoch: 1170, Loss: 0.0009033\n",
      "Epoch: 1171, Loss: 0.0009366\n",
      "Epoch: 1172, Loss: 0.0008600\n",
      "Epoch: 1173, Loss: 0.0008311\n",
      "Epoch: 1174, Loss: 0.0008042\n",
      "Epoch: 1175, Loss: 0.0007062\n",
      "Epoch: 1176, Loss: 0.0007565\n",
      "Epoch: 1177, Loss: 0.0006298\n",
      "Epoch: 1178, Loss: 0.0006787\n",
      "Epoch: 1179, Loss: 0.0005809\n",
      "Epoch: 1180, Loss: 0.0006204\n",
      "Epoch: 1181, Loss: 0.0005898\n",
      "Epoch: 1182, Loss: 0.0005615\n",
      "Epoch: 1183, Loss: 0.0005796\n",
      "Epoch: 1184, Loss: 0.0005288\n",
      "Epoch: 1185, Loss: 0.0005386\n",
      "Epoch: 1186, Loss: 0.0005263\n",
      "Epoch: 1187, Loss: 0.0005106\n",
      "Epoch: 1188, Loss: 0.0005483\n",
      "Epoch: 1189, Loss: 0.0005279\n",
      "Epoch: 1190, Loss: 0.0005992\n",
      "Epoch: 1191, Loss: 0.0006645\n",
      "Epoch: 1192, Loss: 0.0008108\n",
      "Epoch: 1193, Loss: 0.0011066\n",
      "Epoch: 1194, Loss: 0.0016040\n",
      "Epoch: 1195, Loss: 0.0026601\n",
      "Epoch: 1196, Loss: 0.0043451\n",
      "Epoch: 1197, Loss: 0.0075709\n",
      "Epoch: 1198, Loss: 0.0114425\n",
      "Epoch: 1199, Loss: 0.0172215\n",
      "Epoch: 1200, Loss: 0.0174542\n",
      "Epoch: 1201, Loss: 0.0181812\n",
      "Epoch: 1202, Loss: 0.0167069\n",
      "Epoch: 1203, Loss: 0.0334220\n",
      "Epoch: 1204, Loss: 0.0329125\n",
      "Epoch: 1205, Loss: 0.0200805\n",
      "Epoch: 1206, Loss: 0.0239833\n",
      "Epoch: 1207, Loss: 0.0215873\n",
      "Epoch: 1208, Loss: 0.0111421\n",
      "Epoch: 1209, Loss: 0.0149192\n",
      "Epoch: 1210, Loss: 0.0130633\n",
      "Epoch: 1211, Loss: 0.0099979\n",
      "Epoch: 1212, Loss: 0.0115393\n",
      "Epoch: 1213, Loss: 0.0096931\n",
      "Epoch: 1214, Loss: 0.0104243\n",
      "Epoch: 1215, Loss: 0.0091929\n",
      "Epoch: 1216, Loss: 0.0084645\n",
      "Epoch: 1217, Loss: 0.0080431\n",
      "Epoch: 1218, Loss: 0.0071830\n",
      "Epoch: 1219, Loss: 0.0054647\n",
      "Epoch: 1220, Loss: 0.0064034\n",
      "Epoch: 1221, Loss: 0.0048023\n",
      "Epoch: 1222, Loss: 0.0052300\n",
      "Epoch: 1223, Loss: 0.0042122\n",
      "Epoch: 1224, Loss: 0.0043130\n",
      "Epoch: 1225, Loss: 0.0034088\n",
      "Epoch: 1226, Loss: 0.0039936\n",
      "Epoch: 1227, Loss: 0.0028881\n",
      "Epoch: 1228, Loss: 0.0035989\n",
      "Epoch: 1229, Loss: 0.0026917\n",
      "Epoch: 1230, Loss: 0.0028093\n",
      "Epoch: 1231, Loss: 0.0022755\n",
      "Epoch: 1232, Loss: 0.0025040\n",
      "Epoch: 1233, Loss: 0.0019212\n",
      "Epoch: 1234, Loss: 0.0022553\n",
      "Epoch: 1235, Loss: 0.0015609\n",
      "Epoch: 1236, Loss: 0.0019268\n",
      "Epoch: 1237, Loss: 0.0015349\n",
      "Epoch: 1238, Loss: 0.0016090\n",
      "Epoch: 1239, Loss: 0.0012569\n",
      "Epoch: 1240, Loss: 0.0013905\n",
      "Epoch: 1241, Loss: 0.0011713\n",
      "Epoch: 1242, Loss: 0.0013148\n",
      "Epoch: 1243, Loss: 0.0010454\n",
      "Epoch: 1244, Loss: 0.0011658\n",
      "Epoch: 1245, Loss: 0.0010773\n",
      "Epoch: 1246, Loss: 0.0011363\n",
      "Epoch: 1247, Loss: 0.0009393\n",
      "Epoch: 1248, Loss: 0.0009413\n",
      "Epoch: 1249, Loss: 0.0008672\n",
      "Epoch: 1250, Loss: 0.0008420\n",
      "Epoch: 1251, Loss: 0.0007584\n",
      "Epoch: 1252, Loss: 0.0007159\n",
      "Epoch: 1253, Loss: 0.0006748\n",
      "Epoch: 1254, Loss: 0.0006443\n",
      "Epoch: 1255, Loss: 0.0006387\n",
      "Epoch: 1256, Loss: 0.0005908\n",
      "Epoch: 1257, Loss: 0.0005622\n",
      "Epoch: 1258, Loss: 0.0005317\n",
      "Epoch: 1259, Loss: 0.0005334\n",
      "Epoch: 1260, Loss: 0.0005393\n",
      "Epoch: 1261, Loss: 0.0005042\n",
      "Epoch: 1262, Loss: 0.0004872\n",
      "Epoch: 1263, Loss: 0.0004813\n",
      "Epoch: 1264, Loss: 0.0005055\n",
      "Epoch: 1265, Loss: 0.0004937\n",
      "Epoch: 1266, Loss: 0.0005329\n",
      "Epoch: 1267, Loss: 0.0005700\n",
      "Epoch: 1268, Loss: 0.0006683\n",
      "Epoch: 1269, Loss: 0.0008311\n",
      "Epoch: 1270, Loss: 0.0011799\n",
      "Epoch: 1271, Loss: 0.0017629\n",
      "Epoch: 1272, Loss: 0.0028879\n",
      "Epoch: 1273, Loss: 0.0046077\n",
      "Epoch: 1274, Loss: 0.0079134\n",
      "Epoch: 1275, Loss: 0.0113171\n",
      "Epoch: 1276, Loss: 0.0171964\n",
      "Epoch: 1277, Loss: 0.0168397\n",
      "Epoch: 1278, Loss: 0.0154950\n",
      "Epoch: 1279, Loss: 0.0082535\n",
      "Epoch: 1280, Loss: 0.0078280\n",
      "Epoch: 1281, Loss: 0.0105592\n",
      "Epoch: 1282, Loss: 0.0078465\n",
      "Epoch: 1283, Loss: 0.0048380\n",
      "Epoch: 1284, Loss: 0.0066196\n",
      "Epoch: 1285, Loss: 0.0064945\n",
      "Epoch: 1286, Loss: 0.0032537\n",
      "Epoch: 1287, Loss: 0.0044854\n",
      "Epoch: 1288, Loss: 0.0038702\n",
      "Epoch: 1289, Loss: 0.0024930\n",
      "Epoch: 1290, Loss: 0.0040193\n",
      "Epoch: 1291, Loss: 0.0022344\n",
      "Epoch: 1292, Loss: 0.0024322\n",
      "Epoch: 1293, Loss: 0.0031020\n",
      "Epoch: 1294, Loss: 0.0016397\n",
      "Epoch: 1295, Loss: 0.0022727\n",
      "Epoch: 1296, Loss: 0.0020159\n",
      "Epoch: 1297, Loss: 0.0014309\n",
      "Epoch: 1298, Loss: 0.0018921\n",
      "Epoch: 1299, Loss: 0.0013935\n",
      "Epoch: 1300, Loss: 0.0014387\n",
      "Epoch: 1301, Loss: 0.0013565\n",
      "Epoch: 1302, Loss: 0.0011062\n",
      "Epoch: 1303, Loss: 0.0011760\n",
      "Epoch: 1304, Loss: 0.0010128\n",
      "Epoch: 1305, Loss: 0.0009426\n",
      "Epoch: 1306, Loss: 0.0009756\n",
      "Epoch: 1307, Loss: 0.0008172\n",
      "Epoch: 1308, Loss: 0.0007446\n",
      "Epoch: 1309, Loss: 0.0008329\n",
      "Epoch: 1310, Loss: 0.0006588\n",
      "Epoch: 1311, Loss: 0.0006752\n",
      "Epoch: 1312, Loss: 0.0007258\n",
      "Epoch: 1313, Loss: 0.0005353\n",
      "Epoch: 1314, Loss: 0.0006376\n",
      "Epoch: 1315, Loss: 0.0005713\n",
      "Epoch: 1316, Loss: 0.0004953\n",
      "Epoch: 1317, Loss: 0.0005804\n",
      "Epoch: 1318, Loss: 0.0004784\n",
      "Epoch: 1319, Loss: 0.0004634\n",
      "Epoch: 1320, Loss: 0.0004932\n",
      "Epoch: 1321, Loss: 0.0004347\n",
      "Epoch: 1322, Loss: 0.0004522\n",
      "Epoch: 1323, Loss: 0.0004532\n",
      "Epoch: 1324, Loss: 0.0004165\n",
      "Epoch: 1325, Loss: 0.0004595\n",
      "Epoch: 1326, Loss: 0.0004881\n",
      "Epoch: 1327, Loss: 0.0005128\n",
      "Epoch: 1328, Loss: 0.0006092\n",
      "Epoch: 1329, Loss: 0.0007674\n",
      "Epoch: 1330, Loss: 0.0009809\n",
      "Epoch: 1331, Loss: 0.0013715\n",
      "Epoch: 1332, Loss: 0.0019105\n",
      "Epoch: 1333, Loss: 0.0027736\n",
      "Epoch: 1334, Loss: 0.0037904\n",
      "Epoch: 1335, Loss: 0.0053841\n",
      "Epoch: 1336, Loss: 0.0065019\n",
      "Epoch: 1337, Loss: 0.0078382\n",
      "Epoch: 1338, Loss: 0.0080928\n",
      "Epoch: 1339, Loss: 0.0119278\n",
      "Epoch: 1340, Loss: 0.0166969\n",
      "Epoch: 1341, Loss: 0.0333897\n",
      "Epoch: 1342, Loss: 0.0496022\n",
      "Epoch: 1343, Loss: 0.0756937\n",
      "Epoch: 1344, Loss: 0.0689051\n",
      "Epoch: 1345, Loss: 0.0668865\n",
      "Epoch: 1346, Loss: 0.0582791\n",
      "Epoch: 1347, Loss: 0.0537732\n",
      "Epoch: 1348, Loss: 0.0714974\n",
      "Epoch: 1349, Loss: 0.0581839\n",
      "Epoch: 1350, Loss: 0.0888909\n",
      "Epoch: 1351, Loss: 0.0553683\n",
      "Epoch: 1352, Loss: 0.0789490\n",
      "Epoch: 1353, Loss: 0.0763154\n",
      "Epoch: 1354, Loss: 0.0646240\n",
      "Epoch: 1355, Loss: 0.0729446\n",
      "Epoch: 1356, Loss: 0.0507249\n",
      "Epoch: 1357, Loss: 0.0540881\n",
      "Epoch: 1358, Loss: 0.0537606\n",
      "Epoch: 1359, Loss: 0.0380191\n",
      "Epoch: 1360, Loss: 0.0398307\n",
      "Epoch: 1361, Loss: 0.0370119\n",
      "Epoch: 1362, Loss: 0.0312213\n",
      "Epoch: 1363, Loss: 0.0298163\n",
      "Epoch: 1364, Loss: 0.0277429\n",
      "Epoch: 1365, Loss: 0.0240072\n",
      "Epoch: 1366, Loss: 0.0226266\n",
      "Epoch: 1367, Loss: 0.0190129\n",
      "Epoch: 1368, Loss: 0.0192277\n",
      "Epoch: 1369, Loss: 0.0172504\n",
      "Epoch: 1370, Loss: 0.0157477\n",
      "Epoch: 1371, Loss: 0.0137049\n",
      "Epoch: 1372, Loss: 0.0130421\n",
      "Epoch: 1373, Loss: 0.0121888\n",
      "Epoch: 1374, Loss: 0.0106497\n",
      "Epoch: 1375, Loss: 0.0098787\n",
      "Epoch: 1376, Loss: 0.0100224\n",
      "Epoch: 1377, Loss: 0.0081894\n",
      "Epoch: 1378, Loss: 0.0075604\n",
      "Epoch: 1379, Loss: 0.0076130\n",
      "Epoch: 1380, Loss: 0.0067587\n",
      "Epoch: 1381, Loss: 0.0059460\n",
      "Epoch: 1382, Loss: 0.0056862\n",
      "Epoch: 1383, Loss: 0.0054817\n",
      "Epoch: 1384, Loss: 0.0047181\n",
      "Epoch: 1385, Loss: 0.0044069\n",
      "Epoch: 1386, Loss: 0.0043616\n",
      "Epoch: 1387, Loss: 0.0040405\n",
      "Epoch: 1388, Loss: 0.0034258\n",
      "Epoch: 1389, Loss: 0.0033511\n",
      "Epoch: 1390, Loss: 0.0032113\n",
      "Epoch: 1391, Loss: 0.0030290\n",
      "Epoch: 1392, Loss: 0.0026981\n",
      "Epoch: 1393, Loss: 0.0026176\n",
      "Epoch: 1394, Loss: 0.0023906\n",
      "Epoch: 1395, Loss: 0.0022477\n",
      "Epoch: 1396, Loss: 0.0022397\n",
      "Epoch: 1397, Loss: 0.0019899\n",
      "Epoch: 1398, Loss: 0.0018785\n",
      "Epoch: 1399, Loss: 0.0018130\n",
      "Epoch: 1400, Loss: 0.0017229\n",
      "Epoch: 1401, Loss: 0.0016082\n",
      "Epoch: 1402, Loss: 0.0015150\n",
      "Epoch: 1403, Loss: 0.0014552\n",
      "Epoch: 1404, Loss: 0.0013831\n",
      "Epoch: 1405, Loss: 0.0013035\n",
      "Epoch: 1406, Loss: 0.0012687\n",
      "Epoch: 1407, Loss: 0.0011798\n",
      "Epoch: 1408, Loss: 0.0011394\n",
      "Epoch: 1409, Loss: 0.0011048\n",
      "Epoch: 1410, Loss: 0.0010472\n",
      "Epoch: 1411, Loss: 0.0009931\n",
      "Epoch: 1412, Loss: 0.0009692\n",
      "Epoch: 1413, Loss: 0.0009410\n",
      "Epoch: 1414, Loss: 0.0008851\n",
      "Epoch: 1415, Loss: 0.0008664\n",
      "Epoch: 1416, Loss: 0.0008388\n",
      "Epoch: 1417, Loss: 0.0007998\n",
      "Epoch: 1418, Loss: 0.0007810\n",
      "Epoch: 1419, Loss: 0.0007528\n",
      "Epoch: 1420, Loss: 0.0007338\n",
      "Epoch: 1421, Loss: 0.0007080\n",
      "Epoch: 1422, Loss: 0.0006867\n",
      "Epoch: 1423, Loss: 0.0006691\n",
      "Epoch: 1424, Loss: 0.0006496\n",
      "Epoch: 1425, Loss: 0.0006339\n",
      "Epoch: 1426, Loss: 0.0006169\n",
      "Epoch: 1427, Loss: 0.0006023\n",
      "Epoch: 1428, Loss: 0.0005890\n",
      "Epoch: 1429, Loss: 0.0005720\n",
      "Epoch: 1430, Loss: 0.0005588\n",
      "Epoch: 1431, Loss: 0.0005480\n",
      "Epoch: 1432, Loss: 0.0005362\n",
      "Epoch: 1433, Loss: 0.0005245\n",
      "Epoch: 1434, Loss: 0.0005134\n",
      "Epoch: 1435, Loss: 0.0005024\n",
      "Epoch: 1436, Loss: 0.0004929\n",
      "Epoch: 1437, Loss: 0.0004842\n",
      "Epoch: 1438, Loss: 0.0004753\n",
      "Epoch: 1439, Loss: 0.0004669\n",
      "Epoch: 1440, Loss: 0.0004589\n",
      "Epoch: 1441, Loss: 0.0004499\n",
      "Epoch: 1442, Loss: 0.0004428\n",
      "Epoch: 1443, Loss: 0.0004351\n",
      "Epoch: 1444, Loss: 0.0004274\n",
      "Epoch: 1445, Loss: 0.0004206\n",
      "Epoch: 1446, Loss: 0.0004140\n",
      "Epoch: 1447, Loss: 0.0004077\n",
      "Epoch: 1448, Loss: 0.0004013\n",
      "Epoch: 1449, Loss: 0.0003958\n",
      "Epoch: 1450, Loss: 0.0003898\n",
      "Epoch: 1451, Loss: 0.0003845\n",
      "Epoch: 1452, Loss: 0.0003799\n",
      "Epoch: 1453, Loss: 0.0003757\n",
      "Epoch: 1454, Loss: 0.0003745\n",
      "Epoch: 1455, Loss: 0.0003760\n",
      "Epoch: 1456, Loss: 0.0003813\n",
      "Epoch: 1457, Loss: 0.0003920\n",
      "Epoch: 1458, Loss: 0.0004123\n",
      "Epoch: 1459, Loss: 0.0004495\n",
      "Epoch: 1460, Loss: 0.0005152\n",
      "Epoch: 1461, Loss: 0.0006228\n",
      "Epoch: 1462, Loss: 0.0007901\n",
      "Epoch: 1463, Loss: 0.0010556\n",
      "Epoch: 1464, Loss: 0.0014180\n",
      "Epoch: 1465, Loss: 0.0019710\n",
      "Epoch: 1466, Loss: 0.0025446\n",
      "Epoch: 1467, Loss: 0.0033844\n",
      "Epoch: 1468, Loss: 0.0038264\n",
      "Epoch: 1469, Loss: 0.0044123\n",
      "Epoch: 1470, Loss: 0.0039473\n",
      "Epoch: 1471, Loss: 0.0034583\n",
      "Epoch: 1472, Loss: 0.0024862\n",
      "Epoch: 1473, Loss: 0.0019027\n",
      "Epoch: 1474, Loss: 0.0013773\n",
      "Epoch: 1475, Loss: 0.0011411\n",
      "Epoch: 1476, Loss: 0.0011745\n",
      "Epoch: 1477, Loss: 0.0014598\n",
      "Epoch: 1478, Loss: 0.0018231\n",
      "Epoch: 1479, Loss: 0.0016422\n",
      "Epoch: 1480, Loss: 0.0010455\n",
      "Epoch: 1481, Loss: 0.0004641\n",
      "Epoch: 1482, Loss: 0.0005070\n",
      "Epoch: 1483, Loss: 0.0009575\n",
      "Epoch: 1484, Loss: 0.0011667\n",
      "Epoch: 1485, Loss: 0.0009548\n",
      "Epoch: 1486, Loss: 0.0005746\n",
      "Epoch: 1487, Loss: 0.0005020\n",
      "Epoch: 1488, Loss: 0.0006596\n",
      "Epoch: 1489, Loss: 0.0006942\n",
      "Epoch: 1490, Loss: 0.0005306\n",
      "Epoch: 1491, Loss: 0.0003735\n",
      "Epoch: 1492, Loss: 0.0004320\n",
      "Epoch: 1493, Loss: 0.0005985\n",
      "Epoch: 1494, Loss: 0.0006331\n",
      "Epoch: 1495, Loss: 0.0005240\n",
      "Epoch: 1496, Loss: 0.0003916\n",
      "Epoch: 1497, Loss: 0.0003712\n",
      "Epoch: 1498, Loss: 0.0004295\n",
      "Epoch: 1499, Loss: 0.0004298\n",
      "Epoch: 1500, Loss: 0.0003494\n",
      "Epoch: 1501, Loss: 0.0002721\n",
      "Epoch: 1502, Loss: 0.0002784\n",
      "Epoch: 1503, Loss: 0.0003328\n",
      "Epoch: 1504, Loss: 0.0003562\n",
      "Epoch: 1505, Loss: 0.0003261\n",
      "Epoch: 1506, Loss: 0.0002827\n",
      "Epoch: 1507, Loss: 0.0002794\n",
      "Epoch: 1508, Loss: 0.0003176\n",
      "Epoch: 1509, Loss: 0.0003647\n",
      "Epoch: 1510, Loss: 0.0003948\n",
      "Epoch: 1511, Loss: 0.0004152\n",
      "Epoch: 1512, Loss: 0.0004541\n",
      "Epoch: 1513, Loss: 0.0005587\n",
      "Epoch: 1514, Loss: 0.0007439\n",
      "Epoch: 1515, Loss: 0.0010848\n",
      "Epoch: 1516, Loss: 0.0015733\n",
      "Epoch: 1517, Loss: 0.0025087\n",
      "Epoch: 1518, Loss: 0.0038323\n",
      "Epoch: 1519, Loss: 0.0064940\n",
      "Epoch: 1520, Loss: 0.0093030\n",
      "Epoch: 1521, Loss: 0.0144070\n",
      "Epoch: 1522, Loss: 0.0148271\n",
      "Epoch: 1523, Loss: 0.0140544\n",
      "Epoch: 1524, Loss: 0.0064774\n",
      "Epoch: 1525, Loss: 0.0039056\n",
      "Epoch: 1526, Loss: 0.0056744\n",
      "Epoch: 1527, Loss: 0.0051995\n",
      "Epoch: 1528, Loss: 0.0028265\n",
      "Epoch: 1529, Loss: 0.0035345\n",
      "Epoch: 1530, Loss: 0.0042849\n",
      "Epoch: 1531, Loss: 0.0024082\n",
      "Epoch: 1532, Loss: 0.0028598\n",
      "Epoch: 1533, Loss: 0.0033696\n",
      "Epoch: 1534, Loss: 0.0014468\n",
      "Epoch: 1535, Loss: 0.0020553\n",
      "Epoch: 1536, Loss: 0.0024596\n",
      "Epoch: 1537, Loss: 0.0009279\n",
      "Epoch: 1538, Loss: 0.0019090\n",
      "Epoch: 1539, Loss: 0.0017127\n",
      "Epoch: 1540, Loss: 0.0009259\n",
      "Epoch: 1541, Loss: 0.0017259\n",
      "Epoch: 1542, Loss: 0.0010180\n",
      "Epoch: 1543, Loss: 0.0009015\n",
      "Epoch: 1544, Loss: 0.0014089\n",
      "Epoch: 1545, Loss: 0.0006260\n",
      "Epoch: 1546, Loss: 0.0009362\n",
      "Epoch: 1547, Loss: 0.0010448\n",
      "Epoch: 1548, Loss: 0.0006409\n",
      "Epoch: 1549, Loss: 0.0010753\n",
      "Epoch: 1550, Loss: 0.0008943\n",
      "Epoch: 1551, Loss: 0.0010073\n",
      "Epoch: 1552, Loss: 0.0015965\n",
      "Epoch: 1553, Loss: 0.0018402\n",
      "Epoch: 1554, Loss: 0.0032214\n",
      "Epoch: 1555, Loss: 0.0050604\n",
      "Epoch: 1556, Loss: 0.0082509\n",
      "Epoch: 1557, Loss: 0.0111560\n",
      "Epoch: 1558, Loss: 0.0150860\n",
      "Epoch: 1559, Loss: 0.0088238\n",
      "Epoch: 1560, Loss: 0.0030351\n",
      "Epoch: 1561, Loss: 0.0039290\n",
      "Epoch: 1562, Loss: 0.0050106\n",
      "Epoch: 1563, Loss: 0.0044367\n",
      "Epoch: 1564, Loss: 0.0039197\n",
      "Epoch: 1565, Loss: 0.0029883\n",
      "Epoch: 1566, Loss: 0.0032068\n",
      "Epoch: 1567, Loss: 0.0037979\n",
      "Epoch: 1568, Loss: 0.0026334\n",
      "Epoch: 1569, Loss: 0.0028474\n",
      "Epoch: 1570, Loss: 0.0031480\n",
      "Epoch: 1571, Loss: 0.0022488\n",
      "Epoch: 1572, Loss: 0.0026842\n",
      "Epoch: 1573, Loss: 0.0025542\n",
      "Epoch: 1574, Loss: 0.0023361\n",
      "Epoch: 1575, Loss: 0.0025197\n",
      "Epoch: 1576, Loss: 0.0016916\n",
      "Epoch: 1577, Loss: 0.0017352\n",
      "Epoch: 1578, Loss: 0.0015694\n",
      "Epoch: 1579, Loss: 0.0008918\n",
      "Epoch: 1580, Loss: 0.0011584\n",
      "Epoch: 1581, Loss: 0.0008741\n",
      "Epoch: 1582, Loss: 0.0009001\n",
      "Epoch: 1583, Loss: 0.0010733\n",
      "Epoch: 1584, Loss: 0.0009064\n",
      "Epoch: 1585, Loss: 0.0011064\n",
      "Epoch: 1586, Loss: 0.0009156\n",
      "Epoch: 1587, Loss: 0.0009651\n",
      "Epoch: 1588, Loss: 0.0010527\n",
      "Epoch: 1589, Loss: 0.0007273\n",
      "Epoch: 1590, Loss: 0.0008329\n",
      "Epoch: 1591, Loss: 0.0006226\n",
      "Epoch: 1592, Loss: 0.0004872\n",
      "Epoch: 1593, Loss: 0.0005917\n",
      "Epoch: 1594, Loss: 0.0003836\n",
      "Epoch: 1595, Loss: 0.0004554\n",
      "Epoch: 1596, Loss: 0.0004349\n",
      "Epoch: 1597, Loss: 0.0003016\n",
      "Epoch: 1598, Loss: 0.0004312\n",
      "Epoch: 1599, Loss: 0.0003506\n",
      "Epoch: 1600, Loss: 0.0003539\n",
      "Epoch: 1601, Loss: 0.0004334\n",
      "Epoch: 1602, Loss: 0.0003682\n",
      "Epoch: 1603, Loss: 0.0004544\n",
      "Epoch: 1604, Loss: 0.0005119\n",
      "Epoch: 1605, Loss: 0.0005460\n",
      "Epoch: 1606, Loss: 0.0007199\n",
      "Epoch: 1607, Loss: 0.0009417\n",
      "Epoch: 1608, Loss: 0.0012886\n",
      "Epoch: 1609, Loss: 0.0019844\n",
      "Epoch: 1610, Loss: 0.0028095\n",
      "Epoch: 1611, Loss: 0.0044899\n",
      "Epoch: 1612, Loss: 0.0061454\n",
      "Epoch: 1613, Loss: 0.0091169\n",
      "Epoch: 1614, Loss: 0.0099766\n",
      "Epoch: 1615, Loss: 0.0105672\n",
      "Epoch: 1616, Loss: 0.0076165\n",
      "Epoch: 1617, Loss: 0.0072453\n",
      "Epoch: 1618, Loss: 0.0103892\n",
      "Epoch: 1619, Loss: 0.0119579\n",
      "Epoch: 1620, Loss: 0.0101393\n",
      "Epoch: 1621, Loss: 0.0061426\n",
      "Epoch: 1622, Loss: 0.0053813\n",
      "Epoch: 1623, Loss: 0.0046945\n",
      "Epoch: 1624, Loss: 0.0040105\n",
      "Epoch: 1625, Loss: 0.0040023\n",
      "Epoch: 1626, Loss: 0.0029762\n",
      "Epoch: 1627, Loss: 0.0037527\n",
      "Epoch: 1628, Loss: 0.0025609\n",
      "Epoch: 1629, Loss: 0.0022773\n",
      "Epoch: 1630, Loss: 0.0033002\n",
      "Epoch: 1631, Loss: 0.0013103\n",
      "Epoch: 1632, Loss: 0.0024039\n",
      "Epoch: 1633, Loss: 0.0019259\n",
      "Epoch: 1634, Loss: 0.0014062\n",
      "Epoch: 1635, Loss: 0.0022185\n",
      "Epoch: 1636, Loss: 0.0008078\n",
      "Epoch: 1637, Loss: 0.0017277\n",
      "Epoch: 1638, Loss: 0.0010806\n",
      "Epoch: 1639, Loss: 0.0012785\n",
      "Epoch: 1640, Loss: 0.0010649\n",
      "Epoch: 1641, Loss: 0.0008861\n",
      "Epoch: 1642, Loss: 0.0010503\n",
      "Epoch: 1643, Loss: 0.0006700\n",
      "Epoch: 1644, Loss: 0.0009704\n",
      "Epoch: 1645, Loss: 0.0005728\n",
      "Epoch: 1646, Loss: 0.0007528\n",
      "Epoch: 1647, Loss: 0.0005767\n",
      "Epoch: 1648, Loss: 0.0006037\n",
      "Epoch: 1649, Loss: 0.0005673\n",
      "Epoch: 1650, Loss: 0.0004464\n",
      "Epoch: 1651, Loss: 0.0005692\n",
      "Epoch: 1652, Loss: 0.0003518\n",
      "Epoch: 1653, Loss: 0.0004981\n",
      "Epoch: 1654, Loss: 0.0003364\n",
      "Epoch: 1655, Loss: 0.0004246\n",
      "Epoch: 1656, Loss: 0.0003811\n",
      "Epoch: 1657, Loss: 0.0003786\n",
      "Epoch: 1658, Loss: 0.0004555\n",
      "Epoch: 1659, Loss: 0.0004022\n",
      "Epoch: 1660, Loss: 0.0006418\n",
      "Epoch: 1661, Loss: 0.0008298\n",
      "Epoch: 1662, Loss: 0.0014723\n",
      "Epoch: 1663, Loss: 0.0025675\n",
      "Epoch: 1664, Loss: 0.0052002\n",
      "Epoch: 1665, Loss: 0.0092085\n",
      "Epoch: 1666, Loss: 0.0180574\n",
      "Epoch: 1667, Loss: 0.0242695\n",
      "Epoch: 1668, Loss: 0.0330394\n",
      "Epoch: 1669, Loss: 0.0194741\n",
      "Epoch: 1670, Loss: 0.0194260\n",
      "Epoch: 1671, Loss: 0.0308461\n",
      "Epoch: 1672, Loss: 0.0340779\n",
      "Epoch: 1673, Loss: 0.0408657\n",
      "Epoch: 1674, Loss: 0.0382667\n",
      "Epoch: 1675, Loss: 0.0324266\n",
      "Epoch: 1676, Loss: 0.0509386\n",
      "Epoch: 1677, Loss: 0.0304429\n",
      "Epoch: 1678, Loss: 0.0388440\n",
      "Epoch: 1679, Loss: 0.0341210\n",
      "Epoch: 1680, Loss: 0.0361036\n",
      "Epoch: 1681, Loss: 0.0248145\n",
      "Epoch: 1682, Loss: 0.0327407\n",
      "Epoch: 1683, Loss: 0.0241928\n",
      "Epoch: 1684, Loss: 0.0248439\n",
      "Epoch: 1685, Loss: 0.0223041\n",
      "Epoch: 1686, Loss: 0.0215217\n",
      "Epoch: 1687, Loss: 0.0206605\n",
      "Epoch: 1688, Loss: 0.0162316\n",
      "Epoch: 1689, Loss: 0.0150715\n",
      "Epoch: 1690, Loss: 0.0156824\n",
      "Epoch: 1691, Loss: 0.0122049\n",
      "Epoch: 1692, Loss: 0.0122928\n",
      "Epoch: 1693, Loss: 0.0108981\n",
      "Epoch: 1694, Loss: 0.0100237\n",
      "Epoch: 1695, Loss: 0.0088980\n",
      "Epoch: 1696, Loss: 0.0085066\n",
      "Epoch: 1697, Loss: 0.0071382\n",
      "Epoch: 1698, Loss: 0.0069832\n",
      "Epoch: 1699, Loss: 0.0061711\n",
      "Epoch: 1700, Loss: 0.0059508\n",
      "Epoch: 1701, Loss: 0.0050373\n",
      "Epoch: 1702, Loss: 0.0053186\n",
      "Epoch: 1703, Loss: 0.0037477\n",
      "Epoch: 1704, Loss: 0.0042008\n",
      "Epoch: 1705, Loss: 0.0036347\n",
      "Epoch: 1706, Loss: 0.0034836\n",
      "Epoch: 1707, Loss: 0.0027197\n",
      "Epoch: 1708, Loss: 0.0030895\n",
      "Epoch: 1709, Loss: 0.0024155\n",
      "Epoch: 1710, Loss: 0.0023898\n",
      "Epoch: 1711, Loss: 0.0021125\n",
      "Epoch: 1712, Loss: 0.0020931\n",
      "Epoch: 1713, Loss: 0.0017242\n",
      "Epoch: 1714, Loss: 0.0017861\n",
      "Epoch: 1715, Loss: 0.0015592\n",
      "Epoch: 1716, Loss: 0.0014268\n",
      "Epoch: 1717, Loss: 0.0014442\n",
      "Epoch: 1718, Loss: 0.0012240\n",
      "Epoch: 1719, Loss: 0.0011668\n",
      "Epoch: 1720, Loss: 0.0011074\n",
      "Epoch: 1721, Loss: 0.0010272\n",
      "Epoch: 1722, Loss: 0.0009393\n",
      "Epoch: 1723, Loss: 0.0009346\n",
      "Epoch: 1724, Loss: 0.0008028\n",
      "Epoch: 1725, Loss: 0.0008268\n",
      "Epoch: 1726, Loss: 0.0007415\n",
      "Epoch: 1727, Loss: 0.0006854\n",
      "Epoch: 1728, Loss: 0.0006710\n",
      "Epoch: 1729, Loss: 0.0006165\n",
      "Epoch: 1730, Loss: 0.0005973\n",
      "Epoch: 1731, Loss: 0.0005612\n",
      "Epoch: 1732, Loss: 0.0005314\n",
      "Epoch: 1733, Loss: 0.0005054\n",
      "Epoch: 1734, Loss: 0.0004836\n",
      "Epoch: 1735, Loss: 0.0004561\n",
      "Epoch: 1736, Loss: 0.0004464\n",
      "Epoch: 1737, Loss: 0.0004166\n",
      "Epoch: 1738, Loss: 0.0004054\n",
      "Epoch: 1739, Loss: 0.0003923\n",
      "Epoch: 1740, Loss: 0.0003713\n",
      "Epoch: 1741, Loss: 0.0003621\n",
      "Epoch: 1742, Loss: 0.0003482\n",
      "Epoch: 1743, Loss: 0.0003305\n",
      "Epoch: 1744, Loss: 0.0003240\n",
      "Epoch: 1745, Loss: 0.0003182\n",
      "Epoch: 1746, Loss: 0.0003049\n",
      "Epoch: 1747, Loss: 0.0002958\n",
      "Epoch: 1748, Loss: 0.0002864\n",
      "Epoch: 1749, Loss: 0.0002752\n",
      "Epoch: 1750, Loss: 0.0002723\n",
      "Epoch: 1751, Loss: 0.0002662\n",
      "Epoch: 1752, Loss: 0.0002584\n",
      "Epoch: 1753, Loss: 0.0002539\n",
      "Epoch: 1754, Loss: 0.0002454\n",
      "Epoch: 1755, Loss: 0.0002411\n",
      "Epoch: 1756, Loss: 0.0002367\n",
      "Epoch: 1757, Loss: 0.0002318\n",
      "Epoch: 1758, Loss: 0.0002272\n",
      "Epoch: 1759, Loss: 0.0002227\n",
      "Epoch: 1760, Loss: 0.0002184\n",
      "Epoch: 1761, Loss: 0.0002139\n",
      "Epoch: 1762, Loss: 0.0002112\n",
      "Epoch: 1763, Loss: 0.0002069\n",
      "Epoch: 1764, Loss: 0.0002052\n",
      "Epoch: 1765, Loss: 0.0002017\n",
      "Epoch: 1766, Loss: 0.0001984\n",
      "Epoch: 1767, Loss: 0.0001964\n",
      "Epoch: 1768, Loss: 0.0001933\n",
      "Epoch: 1769, Loss: 0.0001924\n",
      "Epoch: 1770, Loss: 0.0001906\n",
      "Epoch: 1771, Loss: 0.0001901\n",
      "Epoch: 1772, Loss: 0.0001912\n",
      "Epoch: 1773, Loss: 0.0001927\n",
      "Epoch: 1774, Loss: 0.0001985\n",
      "Epoch: 1775, Loss: 0.0002076\n",
      "Epoch: 1776, Loss: 0.0002242\n",
      "Epoch: 1777, Loss: 0.0002532\n",
      "Epoch: 1778, Loss: 0.0002971\n",
      "Epoch: 1779, Loss: 0.0003767\n",
      "Epoch: 1780, Loss: 0.0004943\n",
      "Epoch: 1781, Loss: 0.0006922\n",
      "Epoch: 1782, Loss: 0.0009702\n",
      "Epoch: 1783, Loss: 0.0013987\n",
      "Epoch: 1784, Loss: 0.0018533\n",
      "Epoch: 1785, Loss: 0.0024723\n",
      "Epoch: 1786, Loss: 0.0027242\n",
      "Epoch: 1787, Loss: 0.0031081\n",
      "Epoch: 1788, Loss: 0.0029154\n",
      "Epoch: 1789, Loss: 0.0034472\n",
      "Epoch: 1790, Loss: 0.0044266\n",
      "Epoch: 1791, Loss: 0.0067889\n",
      "Epoch: 1792, Loss: 0.0089182\n",
      "Epoch: 1793, Loss: 0.0090919\n",
      "Epoch: 1794, Loss: 0.0054249\n",
      "Epoch: 1795, Loss: 0.0020020\n",
      "Epoch: 1796, Loss: 0.0039840\n",
      "Epoch: 1797, Loss: 0.0044932\n",
      "Epoch: 1798, Loss: 0.0013997\n",
      "Epoch: 1799, Loss: 0.0023951\n",
      "Epoch: 1800, Loss: 0.0027432\n",
      "Epoch: 1801, Loss: 0.0009460\n",
      "Epoch: 1802, Loss: 0.0020651\n",
      "Epoch: 1803, Loss: 0.0014061\n",
      "Epoch: 1804, Loss: 0.0009862\n",
      "Epoch: 1805, Loss: 0.0016845\n",
      "Epoch: 1806, Loss: 0.0007850\n",
      "Epoch: 1807, Loss: 0.0011620\n",
      "Epoch: 1808, Loss: 0.0010720\n",
      "Epoch: 1809, Loss: 0.0007446\n",
      "Epoch: 1810, Loss: 0.0011319\n",
      "Epoch: 1811, Loss: 0.0006321\n",
      "Epoch: 1812, Loss: 0.0008970\n",
      "Epoch: 1813, Loss: 0.0008138\n",
      "Epoch: 1814, Loss: 0.0005963\n",
      "Epoch: 1815, Loss: 0.0008328\n",
      "Epoch: 1816, Loss: 0.0005209\n",
      "Epoch: 1817, Loss: 0.0006649\n",
      "Epoch: 1818, Loss: 0.0007262\n",
      "Epoch: 1819, Loss: 0.0005872\n",
      "Epoch: 1820, Loss: 0.0008513\n",
      "Epoch: 1821, Loss: 0.0007808\n",
      "Epoch: 1822, Loss: 0.0009735\n",
      "Epoch: 1823, Loss: 0.0012326\n",
      "Epoch: 1824, Loss: 0.0014776\n",
      "Epoch: 1825, Loss: 0.0019735\n",
      "Epoch: 1826, Loss: 0.0026759\n",
      "Epoch: 1827, Loss: 0.0032813\n",
      "Epoch: 1828, Loss: 0.0048269\n",
      "Epoch: 1829, Loss: 0.0055045\n",
      "Epoch: 1830, Loss: 0.0074892\n",
      "Epoch: 1831, Loss: 0.0078025\n",
      "Epoch: 1832, Loss: 0.0092405\n",
      "Epoch: 1833, Loss: 0.0097167\n",
      "Epoch: 1834, Loss: 0.0114713\n",
      "Epoch: 1835, Loss: 0.0123640\n",
      "Epoch: 1836, Loss: 0.0148279\n",
      "Epoch: 1837, Loss: 0.0249217\n",
      "Epoch: 1838, Loss: 0.0305162\n",
      "Epoch: 1839, Loss: 0.0426973\n",
      "Epoch: 1840, Loss: 0.0270376\n",
      "Epoch: 1841, Loss: 0.0207544\n",
      "Epoch: 1842, Loss: 0.0264423\n",
      "Epoch: 1843, Loss: 0.0277801\n",
      "Epoch: 1844, Loss: 0.0269548\n",
      "Epoch: 1845, Loss: 0.0302951\n",
      "Epoch: 1846, Loss: 0.0253357\n",
      "Epoch: 1847, Loss: 0.0180542\n",
      "Epoch: 1848, Loss: 0.0193182\n",
      "Epoch: 1849, Loss: 0.0169245\n",
      "Epoch: 1850, Loss: 0.0167913\n",
      "Epoch: 1851, Loss: 0.0126706\n",
      "Epoch: 1852, Loss: 0.0123227\n",
      "Epoch: 1853, Loss: 0.0105911\n",
      "Epoch: 1854, Loss: 0.0091208\n",
      "Epoch: 1855, Loss: 0.0093087\n",
      "Epoch: 1856, Loss: 0.0077458\n",
      "Epoch: 1857, Loss: 0.0072249\n",
      "Epoch: 1858, Loss: 0.0065736\n",
      "Epoch: 1859, Loss: 0.0060523\n",
      "Epoch: 1860, Loss: 0.0055184\n",
      "Epoch: 1861, Loss: 0.0049419\n",
      "Epoch: 1862, Loss: 0.0046173\n",
      "Epoch: 1863, Loss: 0.0040164\n",
      "Epoch: 1864, Loss: 0.0038564\n",
      "Epoch: 1865, Loss: 0.0035436\n",
      "Epoch: 1866, Loss: 0.0029893\n",
      "Epoch: 1867, Loss: 0.0030133\n",
      "Epoch: 1868, Loss: 0.0025972\n",
      "Epoch: 1869, Loss: 0.0023529\n",
      "Epoch: 1870, Loss: 0.0023261\n",
      "Epoch: 1871, Loss: 0.0017807\n",
      "Epoch: 1872, Loss: 0.0021451\n",
      "Epoch: 1873, Loss: 0.0014103\n",
      "Epoch: 1874, Loss: 0.0017632\n",
      "Epoch: 1875, Loss: 0.0013634\n",
      "Epoch: 1876, Loss: 0.0013021\n",
      "Epoch: 1877, Loss: 0.0013510\n",
      "Epoch: 1878, Loss: 0.0010095\n",
      "Epoch: 1879, Loss: 0.0011289\n",
      "Epoch: 1880, Loss: 0.0009517\n",
      "Epoch: 1881, Loss: 0.0008883\n",
      "Epoch: 1882, Loss: 0.0008925\n",
      "Epoch: 1883, Loss: 0.0006927\n",
      "Epoch: 1884, Loss: 0.0008260\n",
      "Epoch: 1885, Loss: 0.0006077\n",
      "Epoch: 1886, Loss: 0.0006909\n",
      "Epoch: 1887, Loss: 0.0005673\n",
      "Epoch: 1888, Loss: 0.0005579\n",
      "Epoch: 1889, Loss: 0.0005388\n",
      "Epoch: 1890, Loss: 0.0004816\n",
      "Epoch: 1891, Loss: 0.0004904\n",
      "Epoch: 1892, Loss: 0.0004243\n",
      "Epoch: 1893, Loss: 0.0004354\n",
      "Epoch: 1894, Loss: 0.0003788\n",
      "Epoch: 1895, Loss: 0.0003809\n",
      "Epoch: 1896, Loss: 0.0003531\n",
      "Epoch: 1897, Loss: 0.0003376\n",
      "Epoch: 1898, Loss: 0.0003314\n",
      "Epoch: 1899, Loss: 0.0003075\n",
      "Epoch: 1900, Loss: 0.0003002\n",
      "Epoch: 1901, Loss: 0.0002780\n",
      "Epoch: 1902, Loss: 0.0002723\n",
      "Epoch: 1903, Loss: 0.0002645\n",
      "Epoch: 1904, Loss: 0.0002538\n",
      "Epoch: 1905, Loss: 0.0002496\n",
      "Epoch: 1906, Loss: 0.0002353\n",
      "Epoch: 1907, Loss: 0.0002327\n",
      "Epoch: 1908, Loss: 0.0002189\n",
      "Epoch: 1909, Loss: 0.0002185\n",
      "Epoch: 1910, Loss: 0.0002053\n",
      "Epoch: 1911, Loss: 0.0002054\n",
      "Epoch: 1912, Loss: 0.0001979\n",
      "Epoch: 1913, Loss: 0.0001939\n",
      "Epoch: 1914, Loss: 0.0001904\n",
      "Epoch: 1915, Loss: 0.0001845\n",
      "Epoch: 1916, Loss: 0.0001839\n",
      "Epoch: 1917, Loss: 0.0001768\n",
      "Epoch: 1918, Loss: 0.0001769\n",
      "Epoch: 1919, Loss: 0.0001704\n",
      "Epoch: 1920, Loss: 0.0001695\n",
      "Epoch: 1921, Loss: 0.0001656\n",
      "Epoch: 1922, Loss: 0.0001643\n",
      "Epoch: 1923, Loss: 0.0001615\n",
      "Epoch: 1924, Loss: 0.0001592\n",
      "Epoch: 1925, Loss: 0.0001600\n",
      "Epoch: 1926, Loss: 0.0001598\n",
      "Epoch: 1927, Loss: 0.0001652\n",
      "Epoch: 1928, Loss: 0.0001719\n",
      "Epoch: 1929, Loss: 0.0001880\n",
      "Epoch: 1930, Loss: 0.0002117\n",
      "Epoch: 1931, Loss: 0.0002469\n",
      "Epoch: 1932, Loss: 0.0003020\n",
      "Epoch: 1933, Loss: 0.0003726\n",
      "Epoch: 1934, Loss: 0.0004808\n",
      "Epoch: 1935, Loss: 0.0005909\n",
      "Epoch: 1936, Loss: 0.0007343\n",
      "Epoch: 1937, Loss: 0.0008419\n",
      "Epoch: 1938, Loss: 0.0009580\n",
      "Epoch: 1939, Loss: 0.0009552\n",
      "Epoch: 1940, Loss: 0.0008818\n",
      "Epoch: 1941, Loss: 0.0007060\n",
      "Epoch: 1942, Loss: 0.0004974\n",
      "Epoch: 1943, Loss: 0.0003788\n",
      "Epoch: 1944, Loss: 0.0003710\n",
      "Epoch: 1945, Loss: 0.0004495\n",
      "Epoch: 1946, Loss: 0.0004881\n",
      "Epoch: 1947, Loss: 0.0004511\n",
      "Epoch: 1948, Loss: 0.0003436\n",
      "Epoch: 1949, Loss: 0.0002400\n",
      "Epoch: 1950, Loss: 0.0002300\n",
      "Epoch: 1951, Loss: 0.0003240\n",
      "Epoch: 1952, Loss: 0.0004554\n",
      "Epoch: 1953, Loss: 0.0005748\n",
      "Epoch: 1954, Loss: 0.0006315\n",
      "Epoch: 1955, Loss: 0.0007195\n",
      "Epoch: 1956, Loss: 0.0008360\n",
      "Epoch: 1957, Loss: 0.0011943\n",
      "Epoch: 1958, Loss: 0.0017590\n",
      "Epoch: 1959, Loss: 0.0028253\n",
      "Epoch: 1960, Loss: 0.0039280\n",
      "Epoch: 1961, Loss: 0.0057261\n",
      "Epoch: 1962, Loss: 0.0060516\n",
      "Epoch: 1963, Loss: 0.0061728\n",
      "Epoch: 1964, Loss: 0.0046754\n",
      "Epoch: 1965, Loss: 0.0057861\n",
      "Epoch: 1966, Loss: 0.0102099\n",
      "Epoch: 1967, Loss: 0.0197070\n",
      "Epoch: 1968, Loss: 0.0345354\n",
      "Epoch: 1969, Loss: 0.0365452\n",
      "Epoch: 1970, Loss: 0.0217468\n",
      "Epoch: 1971, Loss: 0.0235622\n",
      "Epoch: 1972, Loss: 0.0384475\n",
      "Epoch: 1973, Loss: 0.0200386\n",
      "Epoch: 1974, Loss: 0.0306202\n",
      "Epoch: 1975, Loss: 0.0208158\n",
      "Epoch: 1976, Loss: 0.0216068\n",
      "Epoch: 1977, Loss: 0.0199632\n",
      "Epoch: 1978, Loss: 0.0206233\n",
      "Epoch: 1979, Loss: 0.0173518\n",
      "Epoch: 1980, Loss: 0.0181685\n",
      "Epoch: 1981, Loss: 0.0173581\n",
      "Epoch: 1982, Loss: 0.0161764\n",
      "Epoch: 1983, Loss: 0.0153103\n",
      "Epoch: 1984, Loss: 0.0133647\n",
      "Epoch: 1985, Loss: 0.0101842\n",
      "Epoch: 1986, Loss: 0.0135381\n",
      "Epoch: 1987, Loss: 0.0105575\n",
      "Epoch: 1988, Loss: 0.0137874\n",
      "Epoch: 1989, Loss: 0.0094267\n",
      "Epoch: 1990, Loss: 0.0084340\n",
      "Epoch: 1991, Loss: 0.0081658\n",
      "Epoch: 1992, Loss: 0.0073701\n",
      "Epoch: 1993, Loss: 0.0082786\n",
      "Epoch: 1994, Loss: 0.0076791\n",
      "Epoch: 1995, Loss: 0.0081579\n",
      "Epoch: 1996, Loss: 0.0053152\n",
      "Epoch: 1997, Loss: 0.0044008\n",
      "Epoch: 1998, Loss: 0.0054422\n",
      "Epoch: 1999, Loss: 0.0049702\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "data = dataset.to(device)\n",
    "\n",
    "#Train the model\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    num_batches = 0\n",
    "    #for i, data in enumerate(train_loader):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fn(out, data.y)\n",
    "    total_loss += loss.item()\n",
    "    num_batches += 1\n",
    "    #Add to the accuracy the number of correct binary predictions\n",
    "    accuracy += 0#(out.round(decimals=0) == data.y).sum().item()/len(data.y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print('Epoch: {:03d}, Loss: {:.7f}'.format(epoch, loss.item()))\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    # #Find the performance on the test set\n",
    "    # model.eval()\n",
    "    # test_accuracy = 0\n",
    "    # test_loss = 0\n",
    "    # test_num_batches = 0\n",
    "    # for data in test_loader:\n",
    "    #     data = data.to(device)\n",
    "    #     out = model(data)\n",
    "    #     test_loss += loss_fn(out, data.y).item()\n",
    "    #     test_accuracy += 0#(out.round(decimals=0) == data.y).sum().item()/len(data.y)\n",
    "    #     test_num_batches += 1\n",
    "\n",
    "    # print('\\nEpoch: {:03d}, Train Loss: {:.7f}, Train Accuracy: {:.3}, Test Loss: {:.7f}, Test Accuracy: {:.3}'.format(epoch, \n",
    "    #         total_loss/num_batches, accuracy/num_batches, test_loss/test_num_batches, test_accuracy/test_num_batches))\n",
    "\n",
    "    # #Store the results in the dataframe\n",
    "    df = pd.concat([df, pd.DataFrame({'epoch': epoch, 'loss': loss.item()\n",
    "                                        }, index=[0])], ignore_index=True)\n",
    "    # #Save the dataframe to a csv file\n",
    "    df.to_csv('results_forwardpred_single.csv', index=False)\n",
    "\n",
    "    # if(epoch==0):\n",
    "    #     best_loss = test_loss/test_num_batches\n",
    "    # if(test_loss/test_num_batches < best_loss):\n",
    "    #     best_loss = test_loss/test_num_batches\n",
    "    #     torch.save(model.state_dict(), 'model_forwardpred.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
